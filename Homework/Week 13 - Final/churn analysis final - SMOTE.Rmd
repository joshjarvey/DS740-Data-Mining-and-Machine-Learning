---
title: "Untitled"
author: "Josh Jarvey"
date: "11/14/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1.0 - Introduction & Data Prep

In this project, we are interested in modeling, and predicting, customer churn for a banking organization. This dataset was discovered on the Kaggle website located at: https://www.kaggle.com/shrutimechlearn/churn-modelling/. It has a total of 14 columns and 10,000 rows of customer data. 
 
For this first part we begin with reading in the dataset, and preparing the relevant variables for the analysis.

```{r}
churn = read.csv("C:/Users/joshj/Documents/DS740-Data Mining and Machine Learning/Homework/Week 13 - Final/Churn.csv")
```

Next, we remove variables that serve no predictive purpose (index, customer id, name) for this analysis. 

```{r}
churn = churn[,-c(1,2,3)]
```

We also need to change the data types of some of the variables to factors.

```{r}
churn$HasCrCard = as.factor(churn$HasCrCard)
churn$IsActiveMember = as.factor(churn$IsActiveMember)
churn$Exited = as.factor(churn$Exited)
```

Finally, we can check the summary stats of each data type. We're specifically looking for any missing values.

```{r}
summary(churn)
```

There are no missing values in this data set, so we can move on with our next step of exploration.

## 2.0 - Data Exploration

In this section we visually explore our dataset - specifically, we're looking at the distributions of the quantitative variables (any outliers?), and also if there are any relationships between variables. 

```{r, fig.width=20, fig.height=15}
library(ggformula); library(ggpubr)
  #visualize distributions and relationships.
ggarrange(ncol = 2, nrow = 2,
  ggarrange(ncol = 2, nrow = 3,
    gf_histogram(~EstimatedSalary, data = churn, color = ~Exited, fill = ~Exited),
    gf_histogram(~Balance, data = churn, color = ~Exited, fill = ~Exited),
    gf_histogram(~CreditScore, data = churn, color = ~Exited, fill = ~Exited),
    gf_histogram(~Age, data = churn, color = ~Exited, fill = ~Exited),
    gf_histogram(~Tenure,data = churn, color= ~Exited, fill = ~Exited),
    gf_histogram(~NumOfProducts,data = churn, color= ~Exited, fill = ~Exited)),
  ggarrange(ncol = 2, nrow = 3,
    gf_point(EstimatedSalary~Balance, data = churn, color = ~Exited),
    gf_point(EstimatedSalary~CreditScore, data = churn, color = ~Exited),
    gf_point(EstimatedSalary~Age, data = churn, color = ~Exited),
    gf_point(EstimatedSalary~Tenure, data = churn, color = ~Exited),
    gf_point(EstimatedSalary~NumOfProducts, data = churn, color = ~Exited)),
  ggarrange(ncol = 2, nrow = 3,
    gf_boxplot(EstimatedSalary~Exited, data = churn, color = ~Exited),
    gf_boxplot(Balance~Exited, data = churn, color = ~Exited),
    gf_boxplot(CreditScore~Exited, data = churn, color = ~Exited),
    gf_boxplot(Age~Exited, data = churn, color = ~Exited),
    gf_boxplot(Tenure~Exited, data = churn, color = ~Exited),
    gf_boxplot(NumOfProducts~Exited, data = churn, color = ~Exited)))
```

We identify an interesting skew (to the right) in the age variable, therefore we'll use a log() transformation to attempt to make it more normal.

```{r}
churn$Age = log(churn$Age)
```

Although the scatterplots indicate no discernable relationship among the predictor, we also check correlation values between the predictors and confirm there are no significant correlations.

```{r}
(cor(churn[,c(1,4,5,6,7,10)]))
```

## 3.0 - Fitting the model

In this section we evaluate potential model fits using the processed data. First we set up some basic framework using 10-fold cross-validation.

```{r}
n = nrow(churn)
k = 10
  #set various threshold values to test best accuracy error.
threshold = seq(0.01,1, by = 0.01)
  #randomize the data into 10 folds for CV10
groups = c(rep(1:k,floor(n/k)),1:(n-floor(n/k)*k))
set.seed(13, sample.kind = "Rounding")
cvgroups = sample(groups,n) 
```

## 3.1 - Neural Network

Our first candidate model is the neural network. We choose ANN since there is a lot of data available to us, which is where these methods perform well. Secondly, there is doesnt appear to be linearity among our predictors, which is also where ANN's can provide utility. 

A con of the neural network is interpretability. ANN's do not readily provide the influence each variable has on the overall response (i.e. log odds with logistic regression). However, we have made the argument that interpretability isn't a huge priority for this study, and therefore accept less interpretation for greater accuracy in model selection. 

```{r}
library(nnet); library(DMwR)
set.seed(13, sample.kind = "Rounding")
  #setting decay rate and size tuning parameters.
decayRate = seq(.1, 1, by = .1)
size = seq(1,10, by = 1)
  #create storage for predicted prob, 10,000x100 matrix.
nn.predProb = matrix( , nr = n, nc = (length(decayRate)*length(size)))
  #perform CV10 with SMOTE for balancing in test set.
for(i in 1:k){
  groupi = (cvgroups == i)
    #scale numeric predictor variables for train
  nn.train = cbind(churn[!groupi,c(2,3,8,9,11)],
                        scale(churn[!groupi,c(1,4,5,6,7,10)]))
    #calculate mean and sd of train set, stored separately to be used in scaling of validation set.
  nn.train.means = apply(churn[!groupi,c(1,4,5,6,7,10)],2,mean)
  nn.train.sd = apply(churn[!groupi,c(1,4,5,6,7,10)],2,sd)
    #oversampling to balance response class.
  nn.train.SMOTE = SMOTE(Exited~., data = nn.train, perc.over = 200, perc.under = 150, k=5)
    #standardize the validation set using the means and sd's of the train set
  nn.valid = cbind(churn[groupi,c(2,3,8,9,11)],
                  scale(churn[groupi,c(1,4,5,6,7,10)],
                          center = nn.train.means, 
                          scale = nn.train.sd))
    #set index for model number tracking
  nn.index = 1  
  for(s in 1:length(size)){
    for(d in 1:length(decayRate)){
            #fit the model on the train data, and predict the validation data. Store in predprob matrix
        nn.fit = nnet(Exited~., data=nn.train, size = size[s], decay = decayRate[d], maxit = 1000, trace = F) 
        nn.predProb[groupi, nn.index] = predict(nn.fit, nn.valid)
        nn.index = nn.index + 1
    }
  }
}
```

Now that we've completed the training of the neural net, we'll move on to assessing the 100 models performance with a series of various class prediction thresholds. 

```{r}

  #setup storage matrix to calculate accuracies at various thresholds, for each of the 100 ANN models.
nn.model.acc = matrix(,nr = (length(decayRate)*length(size)), nc = length(threshold))
  #loop for threshold values (columns)
for (t in 1:length(threshold)){
    #set predicted class based on current threshold value
  nn.predclass = ifelse(nn.predProb>threshold[t],1,0)
    #loop for model number (rows)
  for (m in 1:(length(size)*length(decayRate))){
      #calculate accuracy for each threshold value and model.
    nn.confusion = table(nn.predclass[,m],churn$Exited)
    nn.model.acc[m,t] = sum(diag(nn.confusion))/sum(nn.confusion)
  }
}
  #Best model: 92 (size 9, decay 0.2), with threshold at 0.57. accuracy = 86.71%. set.seed(13)
which(nn.model.acc == max(nn.model.acc), arr.ind = T); max(nn.model.acc);
  #For the best model, plot the accuracy as a function of threshold. Identify best accuracy with red point. 
plot(threshold,
     nn.model.acc[which(nn.model.acc == max(nn.model.acc), arr.ind = T)[1],],
     type = "l", 
     main = "Neural Network Model Accuracy",xlab = "Threshold Values", ylab = "Accuracy")
points((which(nn.model.acc == max(nn.model.acc), arr.ind = T)[2]/100),max(nn.model.acc),pch = 8,col = "red")

  #We also calculate the AUC for each model, and store it in a vector.
library(pROC)
nn.model.roc = rep(NA, (length(size)*length(decayRate)))
for (m in 1:(length(size)*length(decayRate))){
  nn.model.roc[m] = roc(response=churn$Exited,nn.predProb[,m])$auc
}
  #find the model number with the max AUC.
which(nn.model.roc == max(nn.model.roc), arr.ind = T); max(nn.model.roc);
```

The best model appears to be number 92, which corresponds to the neural network with size of 9 neurons in the hidden layer, and a decay value of 0.2. This provides a maximum accuracy of 86.71% (at the classification threshold of 0.57), and an AUC of 0.867.

Although interpretability is not the top priority in this study, we do review an Olden plot which displays relative importance of each of the predictor variables on the response variable. Olden plot's keep the direction of influence (negative or positive) each variable has on the response. Here we see that: account balance, number of products, and customers located in Spain are the top 3 variables that indicate clients who are more likely to not churn, whereas the variables: is an active member, is male, and from Germany, are the top 3 variables that indicate clients who are more likely to churn. 

```{r, fig.width=20, fig.height=10}
  #scale the numeric predictors
churn.std = cbind(churn[,c(2,3,8,9,11)],scale(churn[,c(1,4,5,6,7,10)]))
  #set the seed and fit the model using the best parameters
set.seed(13, sample.kind = "Rounding")
best.nn.fit = nnet(Exited~., data=churn.std, size = 6, decay = 0.1, maxit = 1000, trace = F) 

  #plot the olden plot to get a sense of variable importance.
library(NeuralNetTools)
olden(best.nn.fit)
```

## 3.2 - Logistic Regression

Our second model of choice is the logistic regression mode. First we must check the assumptions:

```{r}
#this code was selected and modified from the online article:
#http://www.sthda.com/english/articles/36-classification-methods-essentials/148-logistic-regression-assumptions-and-diagnostics-in-r/ 
library(tidyverse)
library(broom)

#1: checking that continuous variables are linear to the logit of the response.
  #full model fit to generate logit values
fullfit = glm(Exited~., data = churn, family = "binomial")
probabilities = predict(fullfit, type = "response")
predicted.classes = ifelse(probabilities>0.5,1,0)
  #only check continuous variables
mydata = churn[,c(1,4,5,6,7,10)]
  #build a stacked dataframe with the logit calculation as a column.
mydata = mydata %>%
  mutate(logit = log(probabilities/(1-probabilities))) %>%
  gather(key = "predictors", value = "predictor.value", -logit)
  #build the scatterplot and display a loess line to check for linearity.
ggplot(mydata, aes(logit, predictor.value))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "loess") + 
  theme_bw() + 
  facet_wrap(~predictors, scales = "free_y")

#2: Check for influential outliers
  #check points on a plot of cooks distance
plot(fullfit, which = 4, id.n = 3)
  #plot residuals
model.data <- augment(fullfit) %>% 
  mutate(index = 1:n()) 
ggplot(model.data, aes(index, .std.resid)) + 
  geom_point(aes(color = Exited), alpha = .5) +
  theme_bw()
  #check if any residuals are greater than 3 (none, so this checks)
model.data %>% 
  filter(abs(.std.resid) > 3)

#3: Check for collinearity. None are above a score of 10.
library(car)
vif(fullfit)

#4: response is binary. OK.
```

All the assumptions for the logistic regression check out, so we can proceed with the predictor selection process and identifying the best model. We use an exhaustive search across all predictors and grade each model by its AIC score - lowest score indicates the best model.

```{r}
library(bestglm)
  #copy dataframe and name response variable as "y" for bestglm()
lr.churn = churn
lr.churn$y = lr.churn$Exited
lr.churn = lr.churn[,-c(11)]
  #find and fit the best logistic regression model per AIC.
best.lr.fit = bestglm(Xy = lr.churn, family = binomial, IC = "AIC", method = "exhaustive")
summary(best.lr.fit$BestModel)
```

We find the best model uses all predictor variables except the ones indicating if the customer has a credit card, and also their estimated salary. Next, we use CV10 with the best model and produce a number of predictions for each observation. 

```{r}
library(DMwR)
  #create storage for predictions.
lr.predProb = rep(-1, n)
  #using CV10, make predicted probabilities for each observation using "best" logistic model.
for(i in 1:k){
  groupi = (cvgroups == i)
    #oversample on the training data to balance the response classes.
  lr.train.SMOTE = SMOTE(Exited~., data = churn[!groupi,], perc.over = 200, perc.under = 150, k=5)
    #fit the best model using the smoted training data. 
  lr.fit = glm(Exited~.-HasCrCard-EstimatedSalary, data = lr.train.SMOTE, family = "binomial")
  lr.predProb[groupi] = predict(lr.fit, churn[groupi,], type = "response")
}
```

After the CV10 predictions are stored, we evaluate the accuracy (number of correct predictions out of the total), and test this at various threshold values. We find that the model with the best accuracy uses a threshold of 0.59, and has an accuracy of 80.95%.

```{r}
library(pROC)
  #creating storage for accuracy calculations
lr.model.acc = rep(-1,length(threshold))
  #for each threshold, calculate and store accuracy of model's predictions
for (t in 1:length(threshold)){
  lr.confusion = table(ifelse(lr.predProb>threshold[t],1,0),churn$Exited)
  lr.model.acc[t] = sum(diag(lr.confusion))/sum(lr.confusion)
}
  #Plot the model's accuracy as a function of the threshold. Best accuracy of 80.95% at threshold 0.59. 
plot(threshold,lr.model.acc,type = "l",main = "Logistic Regression Model Accuracy",xlab = "Threshold Values", ylab = "Accuracy")
points(which.max(lr.model.acc)/100,lr.model.acc[which.max(lr.model.acc)],pch = 8,col = "red")
  #we also plot a roc curve for the best model.
plot.roc(roc(response=churn$Exited,lr.predProb), print.auc = T)
```

Finally, we compare both the neural network and the logistic regression ROC curves on the same plot. It's clear to see that the neural network is the superior model. 

```{r}
  #compare best ROC curves from both the logistic regression and the neural network.
plot.roc(roc(response=churn$Exited,nn.predProb[,61]), col = "green")
plot.roc(roc(response=churn$Exited,lr.predProb), col = "red", add = T)
legend("bottomright", legend=c("Neural Net: AUC = 0.870", "Logistic: AUC = 0.769"),
col=c("green", "red"), lwd=2)
```

