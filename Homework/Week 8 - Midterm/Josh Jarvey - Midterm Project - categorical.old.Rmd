---
title: "Mid-term Low Birthweight Prediction"
author: "Josh Jarvey"
date: "10/26/2020"
output: word_document
---

## 1.0 - Data preparation:

First we will read in our dataset, and check our variable types.

```{r}
  #read in dataset
birth = read.csv("C:/Users/joshj/Documents/DS740-Data Mining and Machine Learning/Homework/Week 8 - Midterm/birthwt.csv")

  #check attribute data types. 
str(birth)
```

Since we will be modeling baby's birth weight lower than 2.5kg (low) variable, we will remove the redundant "bwt" quantitative variable.

```{r}
  #remove the "bwt" variable.
birth = birth[,-2]
```

Some variables will need to be converted to the proper types for analysis.

```{r}
  #convert the following variables to factor type.
birth$low = as.factor(birth$low)
birth$race = as.factor(birth$race)
birth$smoke = as.factor(birth$smoke)
birth$ht = as.factor(birth$ht)
birth$ui = as.factor(birth$ui)
```

Now that we have the variables in the proper format, lets take a look at the summary to see if we have any data that's missing, mis-coded, or extreme outliers that would warrant further investigation.

```{r}
summary(birth)
```

We can also look at histograms/boxplots of the quantitative variables to identify if there are any skews to the observations. 

```{r,message=FALSE, echo=FALSE, fig.width = 10, fig.height = 6}
library(ggformula)
library(ggpubr)

  #create the visualizations
bx_lwt=gf_boxplot(lwt~low, data=birth, color = ~low, xlab="", ylab="", title='Distribution of "lwt" variable')
bx_age=gf_boxplot(age~low, data=birth, color = ~low, xlab="", ylab="", title='Distribution of "age" variable')
hist_lwt = gf_histogram(~lwt, data = birth, color = ~low, fill = ~low, xlab = "", ylab = "")
hist_age = gf_histogram(~age, data = birth, color = ~low, fill = ~low, xlab = "", ylab = "")



  #display the plots 
ggarrange(bx_lwt,bx_age,hist_lwt,hist_age, ncol = 2, nrow = 2)
rm(bx_lwt,bx_age,hist_lwt,hist_age)
```
There does appear to be some skew in the mother's weight variable "lwt", so before we move forward with fitting a model we will perform a log() transformation on the variable, and we'll remove the original from the data set. 

```{r}
  #log transform lwt, and remove it.
birth$loglwt = log(birth$lwt)
birth = birth[,-3]
```

As a final step, we'll check for any signs of collinearity by checking correlations among predictor variables and generate scatterplots to visualize any relationships.

```{r}
  #checking correlation between terms
cor(birth[,c(2,5,8,9)])
  #visualizing any relationships
plot(birth)
```

There are no strong correlations among the set of predictors, therefore we should be able to clear assumptions for our two methods. 

## 2.0 - Data Analysis:

Now that our data is prepared, we can begin the modeling process. Since our variable of interest is a binary response, we will utilize two methods that solve for classification problems: the logistic regression and the random forest. 

For our first candidate model type, we will plan to fit a logistic regression for each of the possible predictor variables. 10-fold cross-validation will be performed on the model, allowing for best model selection among the 8 choices:

############# TESTING BEGIN - 

Here we first set up some of the modeling selection parameters.
  1) we set multiple model by predictors to be used where applicable
  2) we set sample size, and other modeling parameters
  3) Finally, we set 10-fold CV settings, and randomize the observation groupings into 10-folds. 


```{r}
  #setting models.
model1 = (low~age)
model2 = (low~age+race)
model3 = (low~age+race+smoke)
model4 = (low~age+race+smoke+ptl)
model5 = (low~age+race+smoke+ptl+ht)
model6 = (low~age+race+smoke+ptl+ht+ui)
model7 = (low~age+race+smoke+ptl+ht+ui+ftv)
model8 = (low~age+race+smoke+ptl+ht+ui+ftv+loglwt)
allModels = list(model1,model2,model3,model4,model5,model6,model7,model8)

  #setting number of base candidate models
nbaseModels = length(allModels)

  #count the sample size.
n = nrow(birth)

  #set the number of folds.
k = 10

  #create groups of 1-10 for the entire dataset
groups = c(rep(1:k,floor(n/k)),1:(n-floor(n/k)*k)) 
  #set seed for reproduceability
set.seed(8, sample.kind = "Rounding") 
  #randomly shuffle the 1-10 dataset.
cvgroups = sample(groups,n)

```

The logistic regression fitting. First we must check our assumptions about the logistic regression model

```{r}
#this code was selected and modified from the online article:
#http://www.sthda.com/english/articles/36-classification-methods-essentials/148-logistic-regression-assumptions-and-diagnostics-in-r/ 

library(tidyverse)
library(broom)

#1: checking that continuous variables are linear to the logit of the response.
  #full model fit to generate logit values
fullfit = glm(low~., data = birth, family = "binomial")
probabilities = predict(fullfit, type = "response")
predicted.classes = ifelse(probabilities>0.5,1,0)
  #only check continuous variables
mydata = birth[,c(2,9)]
  #build a stacked dataframe with the logit calculation as a column.
mydata = mydata %>%
  mutate(logit = log(probabilities/(1-probabilities))) %>%
  gather(key = "predictors", value = "predictor.value", -logit)
  #build the scatterplot and display a loess line to check for linearity.
ggplot(mydata, aes(logit, predictor.value))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "loess") + 
  theme_bw() + 
  facet_wrap(~predictors, scales = "free_y")

#2: Check for influential outliers
  #check points on a plot of cooks distance
plot(fullfit, which = 4, id.n = 3)
  #plot residuals
model.data <- augment(fullfit) %>% 
  mutate(index = 1:n()) 
ggplot(model.data, aes(index, .std.resid)) + 
  geom_point(aes(color = low), alpha = .5) +
  theme_bw()
  #check if any residuals are greater than 3 (none, so this checks)
model.data %>% 
  filter(abs(.std.resid) > 3)

#3: Check for collinearity. None are above a score of 10.
library(car)
vif(fullfit)

#4: response is binary. OK.
```

now we can fit 8 models, 1 for each predictor. Use cv10 and determine best model fit. 

```{r}
  #create a matrix to store predicted prob results.
lr.allpredictedCV = matrix(rep(NA,n*nbaseModels),ncol=nbaseModels)
  #using CV10, fit all 8 models and make predictions on the test sets.
for (i in 1:k)  {
  test = (cvgroups == i)
  for (m in 1:nbaseModels) {
    lr.fit = glm(formula = allModels[[m]],data = birth[!test,], family = "binomial")
    lr.allpredictedCV[test,m] = predict(lr.fit, birth[test,], type = "response")
  }
}
  #create a vector to store classification error rates
lr.allmisclass = rep(-1,nbaseModels) 
  #calculate mis-class error rates for each of the 8 models.
for(m in 1:nbaseModels){
  cmat = table(ifelse(lr.allpredictedCV[,m]>0.50,"1","0"),birth$low)
  if(dim(cmat)[1] == 2){
    lr.allmisclass[m] = 1-((cmat[1,1] + cmat[2,2]) / sum(cmat))
  }else{
    lr.allmisclass[m] = 1-((cmat[1,1] + 0) / sum(cmat))
  }
}
  #display min mis-class error rate and the associated model number.
which.min(lr.allmisclass); lr.allmisclass[which.min(lr.allmisclass)]
```

The KNN fitting, with multiple k-neighbors (30 models):

  - Seems like a decent option since the predictors arent very linear in relationship, which may throw out off the logistic regression.
  
ANSWER: All predictors with K-neighbors = 3 seems to output best model, with CV10 miss-class rate = 0.1269841

```{r}
library(class)
set.seed(8, sample.kind = "Rounding") 
age = scale(birth$age)
ptl = scale(birth$ptl)
ftv = scale(birth$ftv)
loglwt = scale(birth$loglwt)
k.neighbors = 100

attach(birth)
xvals = data.frame(low,age,race,smoke,ptl,ht,ui,ftv,loglwt)
detach(birth)
rm(age,ptl,ftv,loglwt)

  #create a matrix to store predicted prob results.
knn.allpredictedCV = matrix(rep(NA,n*k.neighbors),ncol=k.neighbors)
  #for CV10
for (i in 1:k)  {
  test = (cvgroups == i)
  train.x = xvals[!test,]
  test.x = xvals[test,]
      #m models for k-neighbors.
  for (m in 1:k.neighbors) {
    knn.allpredictedCV[test,m] = knn(train.x,test.x,birth$low[!test], k = m)
  }
}

  #create a vector to store classification error rates
knn.allmisclass = rep(-1,k.neighbors) 
  #calculate mis-class error rates for each of the 64 models.
for(m in 1:k.neighbors){
  cmat = table(knn.allpredictedCV[,m],birth$low)
  if(dim(cmat)[1] == 2){
    knn.allmisclass[m] = 1-((cmat[1,1] + cmat[2,2]) / sum(cmat))
  }else{
    knn.allmisclass[m] = 1-((cmat[1,1] + 0) / sum(cmat))
  }
}
  #display min mis-class error rate and the associated model number.
which.min(knn.allmisclass); knn.allmisclass[which.min(knn.allmisclass)]

plot(seq(1:k.neighbors),knn.allmisclass, type = "l")
```



The KNN fitting, with multiple predictors and multiple k-neighbors (8 * 30 = 240 models)


ANSWER: Still working on this - seems to not be quite configured correctly.

```{r}
library(class)

age = scale(birth$age)
ptl = scale(birth$ptl)
ftv = scale(birth$ftv)
loglwt = scale(birth$loglwt)

attach(birth)
xvals = data.frame(low,age,race,smoke,ptl,ht,ui,ftv,loglwt)
detach(birth)

  #create a matrix to store predicted prob results.
knn.240.allpredictedCV = matrix(rep(NA,n*knnmodels*30),ncol=knnmodels*30)
  #for CV10
for (i in 1:k)  {
  test = (cvgroups == i)
  train.x = xvals[!test,]
  test.x = xvals[test,]
  index = 1
    #for m models variable selection
  for (m in 1:knnmodels) {
      #for j*m models for k-neighbors parameter.
    for (j in 1:30) {
      knn.240.allpredictedCV[test,index] = knn(train.x[,c(1,2:m)],test.x[,c(1,2:m)],birth$low[!test], k = j)
    	index = index +1
    }
  }
}


  #create a vector to store classification error rates
knn.240.allmisclass = rep(-1,knnmodels*30) 
  #calculate mis-class error rates for each of the 64 models.
for(m in 1:240){
  cmat = table(knn.240.allpredictedCV[,m],birth$low)
  if(dim(cmat)[1] == 2){
    knn.240.allmisclass[m] = 1-((cmat[1,1] + cmat[2,2]) / sum(cmat))
  }else{
    knn.240.allmisclass[m] = 1-((cmat[1,1] + 0) / sum(cmat))
  }
}
  #display min mis-class error rate and the associated model number.
which.min(knn.240.allmisclass); knn.240.allmisclass[which.min(knn.240.allmisclass)]
```


The random forest fitting.

PROS:
  - Random forest because they can handle the outliers (bagging means those observations arent always selected with each tree), and the average tempers them.
  - Random forest because they are indifferent to non-linear features.
  - Random forest because they are resilient to collinearity. But this dataset doesnt seem to have that issue....
  
ANSWER: The model with 1 predictor and an mtry of sqrt(8) gives best CV10 = 0.3174603


```{r}
library(randomForest)
set.seed(8, sample.kind = "Rounding") 
  #create a matrix to store predicted prob results.
rf.allpredictedCV = matrix(rep(NA,n*nbaseModels),ncol=nbaseModels)
  #using CV10, fit all 8 models and make predictions on the test sets.
for (i in 1:k)  {
  test = (cvgroups == i)
  for (m in 1:nbaseModels) {
  	rf.fit = randomForest(formula = allModels[[m]], data = birth[!test,], mtry = sqrt(8))
  	rf.allpredictedCV[test,m] = predict(rf.fit, newdata = birth[test,], type = "prob")[,2]
  }
}
  #create a vector to store classification error rates
rf.allmisclass = rep(-1,nbaseModels)
  #calculate mis-class error rates for each of the 8 models.
for(m in 1:nbaseModels){
  cmat = table(ifelse(rf.allpredictedCV[,m]>0.50,"1","0"),birth$low)
  if(dim(cmat)[1] == 2){
    rf.allmisclass[m] = 1-((cmat[1,1] + cmat[2,2]) / sum(cmat))
  }else{
    rf.allmisclass[m] = 1-((cmat[1,1] + 0) / sum(cmat))
  }
}
  #display min mis-class error rate and the associated model number.
which.min(rf.allmisclass); rf.allmisclass[which.min(rf.allmisclass)]
```

multiple mtry for random forest.

This preformed slightly better than just the predictor selection alone method. 

ANSWER: Best model is model 33 (4 predictors: age+race+smoke+ptl), but mtry = 1. CV10 = 0.2910053

```{r}
library(randomForest)
set.seed(8, sample.kind = "Rounding") 
  #create a matrix to store predicted prob results.
rf.MTRY.allpredictedCV = matrix(rep(NA,n*nbaseModels*nbaseModels),ncol=nbaseModels*nbaseModels)
  #for CV10
for (i in 1:k)  {
  test = (cvgroups == i)
  index = 1
    #for m models variable selection
  for (m in 1:nbaseModels) {
      #for j*m models for mtry.
    for (j in 1:nbaseModels) {
      rf.fit = randomForest(formula = allModels[[m]], data = birth[!test,], mtry = j)
    	rf.MTRY.allpredictedCV[test,index] = predict(rf.fit, newdata = birth[test,], type = "prob")[,2]#pred class=1 (match log reg)
    	index = index +1
    }
  }
}
  #create a vector to store classification error rates
rf.MTRY.allmisclass = rep(-1,nbaseModels*nbaseModels) 
  #calculate mis-class error rates for each of the 64 models.
for(m in 1:64){
  cmat = table(ifelse(rf.MTRY.allpredictedCV[,m]>0.50,"1","0"),birth$low)
  if(dim(cmat)[1] == 2){
    rf.MTRY.allmisclass[m] = 1-((cmat[1,1] + cmat[2,2]) / sum(cmat))
  }else{
    rf.MTRY.allmisclass[m] = 1-((cmat[1,1] + 0) / sum(cmat))
  }
}
  #display min mis-class error rate and the associated model number.
which.min(rf.MTRY.allmisclass); rf.MTRY.allmisclass[which.min(rf.MTRY.allmisclass)]
```

The boost model

```{r}
library(gbm)
  #reload the dataset and leave response variable as integer for boost model.
birth = read.csv("C:/Users/joshj/Documents/DS740-Data Mining and Machine Learning/Homework/Week 8 - Midterm/birthwt.csv")
birth = birth[,-2]
birth$race = as.factor(birth$race)
birth$smoke = as.factor(birth$smoke)
birth$ht = as.factor(birth$ht)
birth$ui = as.factor(birth$ui)
birth$loglwt = log(birth$lwt)
birth = birth[,-3]

  #create a matrix to store predicted results from the CV10 for each of the 8 Logistic Regression models.
boost.allpredictedCV = matrix(rep(NA,n*nbaseModels),ncol=nbaseModels)

for (i in 1:k)  {
  test = (cvgroups == i)
  
  #fit each of the logistic regression models on training, and predict the test
  for (m in 1:nbaseModels) {
    boost.fit = gbm(formula = allModels[[m]],
                    data = birth[!test,],
                    distribution = "bernoulli",
                    n.trees = 1000,
                    shrinkage = 0.001,
                    interaction.depth = 3)
	  boost.allpredictedCV[test,m] = predict(boost.fit, newdata = birth[test,], n.trees = 1000, type = "response")
  }
}

  #create a vector to store classification error rates
boost.allmisclass = rep(-1,nbaseModels) 
  #calculate mis-class error rates for each of the 64 models.
for(m in 1:nbaseModels){
  cmat = table(ifelse(boost.allpredictedCV[,m]>0.50,"1","0"),birth$low)
  if(dim(cmat)[1] == 2){
    boost.allmisclass[m] = 1-((cmat[1,1] + cmat[2,2]) / sum(cmat))
  }else{
    boost.allmisclass[m] = 1-((cmat[1,1] + 0) / sum(cmat))
  }
}
  #display min mis-class error rate and the associated model number.
which.min(boost.allmisclass); boost.allmisclass[which.min(boost.allmisclass)]

```


The multivariate LDA/QDA classification fitting. 

Assumptions:
  - Must be multivariate normal: (check test hztest from MVN package)
  - Must have equal covariance matricies (check boxM from biotools package). If not, then need to use QDA.
    - QDA has a lower bias, but higher variance

Pros:

Cons:
  - There needs to actually be seperatation between classes, which there isnt much...
 
ANSWER: lowest CV appears to be model 5 with CV10 = 0.3068783

```{r}
library(MVN)
library(biotools)
library(MASS)

xvar = cbind(birth$age,birth$loglwt)

xlow = xvar[birth$low == 1,]
xnorm = xvar[birth$low == 0,]
  #does not appear to be mv normal
mvn(xlow, mvnTest = "hz")
mvn(xnorm, mvnTest = "hz")

  #not enough evidence to reject equal covariance matrix.
boxM(xvar,birth$low)



  #create a matrix to store predicted prob results.
lda.allpredictedCV = matrix(rep(NA,n*nbaseModels),ncol=nbaseModels)
  #using CV10, fit all 8 models and make predictions on the test sets.
for (i in 1:k)  {
  test = (cvgroups == i)
  for (m in 1:nbaseModels) {
    lda.fit = lda(formula = allModels[[m]],data = birth[!test,])
    lda.allpredictedCV[test,m] = predict(lda.fit, birth[test,])$class
  }
}
  #create a vector to store classification error rates
lda.allmisclass = rep(-1,nbaseModels) 
  #calculate mis-class error rates for each of the 8 models.
for(m in 1:nbaseModels){
  cmat = table(lda.allpredictedCV[,m],birth$low)
  if(dim(cmat)[1] == 2){
    lda.allmisclass[m] = 1-((cmat[1,1] + cmat[2,2]) / sum(cmat))
  }else{
    lda.allmisclass[m] = 1-((cmat[1,1] + 0) / sum(cmat))
  }
}
  #display min mis-class error rate and the associated model number.
which.min(lda.allmisclass); lda.allmisclass[which.min(lda.allmisclass)]
```

Shouldnt need QDA model here since the covariance matrix can both be assumed equal (BoxM test pvalue = ~0.30), but fitting as a test.

ANSWER: best model is 1, and CV10 = 0.3121693


```{r}

  #create a matrix to store predicted prob results.
qda.allpredictedCV = matrix(rep(NA,n*nbaseModels),ncol=nbaseModels)
  #using CV10, fit all 8 models and make predictions on the test sets.
for (i in 1:k)  {
  test = (cvgroups == i)
  for (m in 1:nbaseModels) {
    qda.fit = qda(formula = allModels[[m]],data = birth[!test,])
    qda.allpredictedCV[test,m] = predict(qda.fit, birth[test,])$class
  }
}
  #create a vector to store classification error rates
qda.allmisclass = rep(-1,nbaseModels) 
  #calculate mis-class error rates for each of the 8 models.
for(m in 1:nbaseModels){
  cmat = table(qda.allpredictedCV[,m],birth$low)
  if(dim(cmat)[1] == 2){
    qda.allmisclass[m] = 1-((cmat[1,1] + cmat[2,2]) / sum(cmat))
  }else{
    qda.allmisclass[m] = 1-((cmat[1,1] + 0) / sum(cmat))
  }
}
  #display min mis-class error rate and the associated model number.
which.min(qda.allmisclass); qda.allmisclass[which.min(qda.allmisclass)]
```


###############TESTING END~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~




Now that we have completed the 10-fold cross-validation for each of the 8 models, we will create a ROC plot containing each of the models. 


```{r}
library(pROC)

  #creating the roc curve objects.
lr1.roc = roc(response = birth$low, predictor = lr1.predvals)
lr2.roc = roc(response = birth$low, predictor = lr2.predvals)
lr3.roc = roc(response = birth$low, predictor = lr3.predvals)
lr4.roc = roc(response = birth$low, predictor = lr4.predvals)
lr5.roc = roc(response = birth$low, predictor = lr5.predvals)
lr6.roc = roc(response = birth$low, predictor = lr6.predvals)
lr7.roc = roc(response = birth$low, predictor = lr7.predvals)
lr8.roc = roc(response = birth$low, predictor = lr8.predvals)

  #plotting the roc curves onto one plot
plot(lr1.roc)
plot(lr2.roc, col="blue", add = T)
plot(lr3.roc, col="red", add = T)
plot(lr4.roc, col="yellow", add = T)
plot(lr5.roc, col="brown", add = T)
plot(lr6.roc, col="green", add = T)
plot(lr7.roc, col="orange", add = T)
plot(lr8.roc, col="purple", add = T)
legend("topleft", 
       legend = c("Model1", "Model2","Model3", "Model4","Model5", "Model6","Model7", "Model8"), 
       pch = c(21, 21, 21, 21,21, 21, 21, 21), 
       col = c("black","blue","red","yellow","brown","green","orange","purple"))

  #calculating the AUC of each roc curve.
lr.AUC=list(lr1.roc$auc,lr2.roc$auc,lr3.roc$auc,lr4.roc$auc,lr5.roc$auc,lr6.roc$auc,lr7.roc$auc,lr8.roc$auc)

which.max(lr.AUC)
lr.AUC[which.max(lr.AUC)]

```

Per the above ROC curves plot and AUC calculations, model 8 (containing all predictors) provides the best fitting model with an AUC of 0.679.

Our next candidate model will be a random forest classification model. Again, we fit a random forest for each of the possible predictor variables. 10-fold cross-validation is performed on the model, allowing for best model selection among the 8 choices:

```{r}
  #set sample size.
n = nrow(birth)
  #set the number of folds.
k = 10

  #create groups of 1-10 for the entire dataset
groups = c(rep(1:k,floor(n/k)),1:(n-floor(n/k)*k)) 
  #set seed for reproduceability
set.seed(8, sample.kind = "Rounding") 
  #randomly shuffle the 1-10 dataset.
cvgroups = sample(groups,n)

  #initializing an empty vector to store the CV error. set to -1 by default for easier troubleshooting.
rf1.predvals = rep(-1,n)
rf2.predvals = rep(-1,n)
rf3.predvals = rep(-1,n)
rf4.predvals = rep(-1,n)
rf5.predvals = rep(-1,n)
rf6.predvals = rep(-1,n)
rf7.predvals = rep(-1,n)
rf8.predvals = rep(-1,n)

  #setting up a for loop to perform cross-validation
for(i in 1:k){
    #using the current iteration of the CV fold, set up a "test" hold out sample.
  test = (cvgroups == i)
  
    #perform random forest using just the training data.
	rf.fit1 = randomForest(low~age, data = birth[train,], mtry = sqrt(8))
  rf.fit2 = randomForest(low~age+race, data = birth[train,], mtry = sqrt(8))
	rf.fit3 = randomForest(low~age+race+smoke, data = birth[train,], mtry = sqrt(8))
	rf.fit4 = randomForest(low~age+race+smoke+ptl, data = birth[train,], mtry = sqrt(8))
	rf.fit5 = randomForest(low~age+race+smoke+ptl+ht, data = birth[train,], mtry = sqrt(8))
	rf.fit6 = randomForest(low~age+race+smoke+ptl+ht+ui, data = birth[train,], mtry = sqrt(8))
	rf.fit7 = randomForest(low~age+race+smoke+ptl+ht+ui+ftv, data = birth[train,], mtry = sqrt(8))
	rf.fit8 = randomForest(low~age+race+smoke+ptl+ht+ui+ftv+loglwt, data = birth[train,], mtry = sqrt(8))
	
	  #using the fitted models, make predictions and store those 
	rf1.predvals[test] = predict(rf.fit1, newdata = birth[test,], type = "prob")
  rf2.predvals[test] = predict(rf.fit2, newdata = birth[test,], type = "prob")
	rf3.predvals[test] = predict(rf.fit3, newdata = birth[test,], type = "prob")
	rf4.predvals[test] = predict(rf.fit4, newdata = birth[test,], type = "prob")
	rf5.predvals[test] = predict(rf.fit5, newdata = birth[test,], type = "prob")
	rf6.predvals[test] = predict(rf.fit6, newdata = birth[test,], type = "prob")
  rf7.predvals[test] = predict(rf.fit7, newdata = birth[test,], type = "prob")
	rf8.predvals[test] = predict(rf.fit8, newdata = birth[test,], type = "prob")
}

```

As with the logistic regression, we generate the ROC plot to review model performance. 

```{r}
library(pROC)
  #creating the roc objects.
rf1.roc = roc(response = birth$low, predictor = rf1.predvals)
rf2.roc = roc(response = birth$low, predictor = rf2.predvals)
rf3.roc = roc(response = birth$low, predictor = rf3.predvals)
rf4.roc = roc(response = birth$low, predictor = rf4.predvals)
rf5.roc = roc(response = birth$low, predictor = rf5.predvals)
rf6.roc = roc(response = birth$low, predictor = rf6.predvals)
rf7.roc = roc(response = birth$low, predictor = rf7.predvals)
rf8.roc = roc(response = birth$low, predictor = rf8.predvals)

  #plotting the roc curves
plot(rf1.roc)
plot(rf2.roc, col="blue", add = T)
plot(rf3.roc, col="red", add = T)
plot(rf4.roc, col="yellow", add = T)
plot(rf5.roc, col="brown", add = T)
plot(rf6.roc, col="green", add = T)
plot(rf7.roc, col="orange", add = T)
plot(rf8.roc, col="purple", add = T)
legend("topleft", 
       legend = c("Model1", "Model2","Model3", "Model4","Model5", "Model6","Model7", "Model8"), 
       pch = c(21, 21, 21, 21,21, 21, 21, 21), 
       col = c("black","blue","red","yellow","brown","green","orange","purple"))

  #calculating the AUC of each roc curve.
rf.AUC=list(rf1.roc$auc,rf2.roc$auc,rf3.roc$auc,rf4.roc$auc,rf5.roc$auc,rf6.roc$auc,rf7.roc$auc,rf8.roc$auc)

which.max(rf.AUC)
rf.AUC[which.max(rf.AUC)]
```

In the case of random forest, we find much better performance, with an overall model #8 (containing all predictor variables) as our top choice. It has an AUC of 0.9357.



Finally, i repeat this process once more with boosting:


```{r}
library(gbm)

  #reload the dataset and leave response variable as integer for boost model.
birth = read.csv("C:/Users/joshj/Documents/DS740-Data Mining and Machine Learning/Homework/Week 8 - Midterm/birthwt.csv")
birth = birth[,-2]
birth$race = as.factor(birth$race)
birth$smoke = as.factor(birth$smoke)
birth$ht = as.factor(birth$ht)
birth$ui = as.factor(birth$ui)
birth$loglwt = log(birth$lwt)
birth = birth[,-3]




  #set sample size.
n = nrow(birth)
  #set the number of folds.
k = 10

  #create groups of 1-10 for the entire dataset
groups = c(rep(1:k,floor(n/k)),1:(n-floor(n/k)*k)) 
  #set seed for reproduceability
set.seed(8, sample.kind = "Rounding") 
  #randomly shuffle the 1-10 dataset.
cvgroups = sample(groups,n)

  #initializing an empty vector to store the CV error. set to -1 by default for easier troubleshooting.
boost1.predvals = rep(-1,n)
boost2.predvals = rep(-1,n)
boost3.predvals = rep(-1,n)
boost4.predvals = rep(-1,n)
boost5.predvals = rep(-1,n)
boost6.predvals = rep(-1,n)
boost7.predvals = rep(-1,n)
boost8.predvals = rep(-1,n)

  #setting up a for loop to perform cross-validation
for(i in 1:k){
    #using the current iteration of the CV fold, set up a "test" hold out sample.
  test = (cvgroups == i)
  
    #perform random forest using just the training data.
  boost.fit1 = gbm(low~age, data = birth[!test,], distribution = "bernoulli", n.trees = 1000,shrinkage = 0.001, interaction.depth = 3)
  boost.fit2 = gbm(low~age+race, data = birth[!test,], distribution = "bernoulli", n.trees = 1000,shrinkage = 0.001, interaction.depth = 3)
	boost.fit3 = gbm(low~age+race+smoke, data = birth[!test,], distribution = "bernoulli", n.trees = 1000,shrinkage = 0.001, interaction.depth = 3)
	boost.fit4 = gbm(low~age+race+smoke+ptl, data = birth[!test,], distribution = "bernoulli", n.trees = 1000,shrinkage = 0.001, interaction.depth = 3)
	boost.fit5 = gbm(low~age+race+smoke+ptl+ht, data = birth[!test,], distribution = "bernoulli", n.trees = 1000,shrinkage = 0.001, interaction.depth = 3)
	boost.fit6 = gbm(low~age+race+smoke+ptl+ht+ui, data = birth[!test,], distribution = "bernoulli", n.trees = 1000,shrinkage = 0.001, interaction.depth = 3)
	boost.fit7 = gbm(low~age+race+smoke+ptl+ht+ui+ftv, data = birth[!test,], distribution = "bernoulli", n.trees = 1000,shrinkage = 0.001, interaction.depth = 3)
	boost.fit8 = gbm(low~age+race+smoke+ptl+ht+ui+ftv+loglwt, data = birth[!test,], distribution = "bernoulli", n.trees = 1000,shrinkage = 0.001, interaction.depth = 3)
	
	  #using the fitted models, make predictions and store those 
	boost1.predvals[test] = predict(boost.fit1, newdata = birth[test,], n.trees = 1000, type = "response")
  boost2.predvals[test] = predict(boost.fit2, newdata = birth[test,], n.trees = 1000, type = "response")
	boost3.predvals[test] = predict(boost.fit3, newdata = birth[test,], n.trees = 1000, type = "response")
	boost4.predvals[test] = predict(boost.fit4, newdata = birth[test,], n.trees = 1000, type = "response")
	boost5.predvals[test] = predict(boost.fit5, newdata = birth[test,], n.trees = 1000, type = "response")
	boost6.predvals[test] = predict(boost.fit6, newdata = birth[test,], n.trees = 1000, type = "response")
  boost7.predvals[test] = predict(boost.fit7, newdata = birth[test,], n.trees = 1000, type = "response")
	boost8.predvals[test] = predict(boost.fit8, newdata = birth[test,], n.trees = 1000, type = "response")
}



```

And now we create the ROC plot for boosting:

```{r}
library(pROC)
  #creating the roc objects.
boost1.roc = roc(response = birth$low, predictor = boost1.predvals)
boost2.roc = roc(response = birth$low, predictor = boost2.predvals)
boost3.roc = roc(response = birth$low, predictor = boost3.predvals)
boost4.roc = roc(response = birth$low, predictor = boost4.predvals)
boost5.roc = roc(response = birth$low, predictor = boost5.predvals)
boost6.roc = roc(response = birth$low, predictor = boost6.predvals)
boost7.roc = roc(response = birth$low, predictor = boost7.predvals)
boost8.roc = roc(response = birth$low, predictor = boost8.predvals)

  #plotting the roc curves
plot(boost1.roc)
plot(boost2.roc, col="blue", add = T)
plot(boost3.roc, col="red", add = T)
plot(boost4.roc, col="yellow", add = T)
plot(boost5.roc, col="brown", add = T)
plot(boost6.roc, col="green", add = T)
plot(boost7.roc, col="orange", add = T)
plot(boost8.roc, col="purple", add = T)
legend("topleft", 
       legend = c("Model1", "Model2","Model3", "Model4","Model5", "Model6","Model7", "Model8"), 
       pch = c(21, 21, 21, 21,21, 21, 21, 21), 
       col = c("black","blue","red","yellow","brown","green","orange","purple"))

  #calculating the AUC of each roc curve.
boost.AUC=list(boost1.roc$auc,boost2.roc$auc,boost3.roc$auc,boost4.roc$auc,boost5.roc$auc,boost6.roc$auc,boost7.roc$auc,boost8.roc$auc)

which.max(boost.AUC)
boost.AUC[which.max(boost.AUC)]
```

And we have a best model at model number 7 (-loglwt), with an AUC of 0.6433.













PERFORM 2x CV

```{r}

nmodels2 = nbaseModels+k.neighbors

###################################################################
##### Double cross-validation for modeling-process assessment #####				 
###################################################################


#############################################
##### General prep for outer CV process #####
#############################################

  #select the full dataset into a variables
fulldata.out = birth
  #set the number of Outer folds for our 2x CV.
k.out = 10 
  #count the number of samples in the full dataset.
n.out = nrow(fulldata.out)
  #setting up the list of indices for CV groups based on the dataset. 
  #this first line replicates 1:number of folds, an even amount of times. 
groups.out = c(rep(1:k.out,floor(n.out/k.out)))
  #however, that might not always work where the data set is even, and if so, append the "remainder" to the indice group. 
if(floor(n.out/k.out) != (n.out/k.out)){
  groups.out = c(groups.out, 1:(n.out%%k.out))
}

  #set seed for reproducability
set.seed(8, sample.kind = "Rounding")
  #randomize the indices using sample()
cvgroups.out = sample(groups.out,n.out)

  #set up storage for predicted values from the double-cross-validation
allpredictedCV.out = rep(NA,n.out)
  #set up storage to see what models are "best" on the inner loops
allbestmodels = rep(NA,k.out)

####################################
###   Begin double-CV process    ###
####################################
for (j in 1:k.out){ 
  if(j == 2){
    break
  }
  ################################################################################
  ##                    ...Prep for OUTER CV process...                         ##
  ## (splitting the full dataset into a train/test split for current iteration) ##
  ##        (j'th training split will get passed into inner CV process)         ##
  ################################################################################
  #lets find the indices of the current outer fold's iteration. So if j=1, then find where all the 1's are in the randomized indice set
  groupj.out = (cvgroups.out == j)

  #using this vector as described above, select everything else that's NOT in current fold - this is our "training" data.
  traindata.out = birth[!groupj.out,]
    #using this vector as described above, select ONLY the current fold indices - this is our "test" data
  validdata.out = birth[groupj.out,]
  
  
    #from the training data set, we can create the matrix of x's for training
  trainx.out = model.matrix(low~.,data=traindata.out)[,-1]
    #from the testing data set, we can create the matrix of x's for testing
  validx.out = model.matrix(low~.,data=validdata.out)[,-1]
    #from the training data set, we can create the vector of y's for training
  trainy.out = traindata.out[,1]
    #from the testing data set, we can create the vector of y's for testing
  validy.out = validdata.out[,1]
  
  ####################################
  ### pipe current training data   ###
  ####################################
  ########	:	:	:	:	:	:	:  ###########
  
    #now that we've split up our outer CV10 data, we can pass in the current iterations "training" dataset
    #if this is iteration j=1, then traindata.out is all of the data from folds 9-10. etc.
  fulldata.in = traindata.out
  
  ########	:	:	:	:	:	:	:  ###########
  ####################################
  ####  start inner CV process    ####
  ####################################
  
  ################################################################################
  ##                    ...Prep for INNER CV process...                         ##
  ## (splitting piped in data set (train from outer) into train/test split)     ##
  ################################################################################
  
    #number folds and groups for (inner) cross-validation for model-selection
  k.in = 10   
    #set sample size of the "inner" dataset. 
  n.in = nrow(fulldata.in)
    #set the indices of the inner groups 
  groups.in = c(rep(1:k.in,floor(n.in/k.in))) 
    #if the groups do not divide evenly, then append the remainder of indices onto the vector
  if(floor(n.in/k.in) != (n.in/k.in)){
    groups.in = c(groups.in, 1:(n.in%%k.in))  
  }
    #now we must randomize the indices of this inner training set.
  cvgroups.in = sample(groups.in,n.in)
  
    #place-holder for results of CV values from each model. 
    #There are 8 logistic, and 100 knn models - so 108 in total
  allmodelCV.in = rep(NA,nmodels2)  

  ##########################################
  #### start inner CV for linear models  ###
  ##########################################
  
    #############################################
    ### start inner CV for logistic regression ##
    #############################################
  #since linear regression does not have any automatic CV output,
  #set up storage for predicted values from the CV splits, across all linear models.
  lr.allpredictedCV.in = matrix(rep(NA,n.in*nbaseModels),ncol=nbaseModels)
  
  #cycle through all folds:  fit the model to training data, predict test data,
  # and store the (cross-validated) predicted values
  for (i in 1:k.in)  {
    train.in = (cvgroups.in != i)
    test.in = (cvgroups.in == i)
    #fit each of the linear regression models on training, and predict the test
    for (m in 1:nbaseModels) {
      lr.fitCV.in = glm(formula = allModels[[m]],data = birth, subset = train.in, family = "binomial")
      lr.allpredictedCV.in[test.in,m] = predict(lr.fitCV.in, fulldata.in[test.in,], type = "response")
    }
  }
  #compute and store the CV(10) values
  lr.allCV.in = rep(-1,nbaseModels)
  for (m in 1:nbaseModels) { 
    cmat = table(ifelse(lr.allpredictedCV[,m]>0.50,"1","0"),birth$low)
    if(dim(cmat)[1] == 2){
      lr.allCV.in[m] = 1-((cmat[1,1] + cmat[2,2]) / sum(cmat))
    }else{
      lr.allCV.in[m] = 1-((cmat[1,1] + 0) / sum(cmat))
    }
  }
   ###########################################
   ### end inner CV for logistic regression ##
   ###########################################
  
   #############################################
   ### start inner CV for k-nearest neighbors ##
   #############################################

  age = scale(fulldata.in$age)
  ptl = scale(fulldata.in$ptl)
  ftv = scale(fulldata.in$ftv)
  loglwt = scale(fulldata.in$loglwt)
  
  attach(fulldata.in)
  xvals.in = data.frame(low,age,race,smoke,ptl,ht,ui,ftv,loglwt)
  detach(fulldata.in)
  rm(age,ptl,ftv,loglwt)
  
  #create a matrix to store predicted prob results.
  knn.allpredictedCV.in = matrix(rep(NA,n.in*k.neighbors),ncol=k.neighbors)
    #for CV10
  for (i in 1:k.in)  {
    test.in = (cvgroups.in == i)
    train.x.in = xvals.in[!test.in,]
    test.x.in = xvals.in[test.in,]
        #m models for (30) k-neighbors.
    for (m in 1:k.neighbors) {
      knn.allpredictedCV.in[test.in,m] = knn(train.x.in,test.x.in,fulldata.in$low[!test.in], k = m)
    }
  }
  
  knn.allCV.in = rep(-1,k.neighbors)
    #calculate mis-class error rates for each of the 100 models.
  for(m in 1:k.neighbors){
    cmat = table(knn.allpredictedCV[,m],birth$low)
    if(dim(cmat)[1] == 2){
      knn.allCV.in[m] = 1-((cmat[1,1] + cmat[2,2]) / sum(cmat))
    }else{
      knn.allCV.in[m] = 1-((cmat[1,1] + 0) / sum(cmat))
    }
  }
    #combining CV10 errors from lr and knn.
  allmodelCV.in = c(lr.allCV.in,knn.allCV.in)
   ###########################################
   ### end inner CV for k-nearest neighbors ##
   ###########################################
  
  ############################################
  ## End model fitting inner CV calculation ##
  ############################################  
  
  #######################################################################
  ##                          identify best model                      ##
  ## (set it to bestmodel, and set best hyper parameters if available) ##
  #######################################################################  
  
      #now that we've fit all the models, pick the best one.
  bestmodel.in = order(allmodelCV.in)[1]

  ### finally, fit the best model to the full (available. i.e. outer CV's training set) data. 
  #if the model was a penalized regression, then use the tuned hyper parameters as well.
  if (bestmodel.in <= nbaseModels) {  # then best is one of logistic models
    bestfit = glm(formula = allModels[[bestmodel.in]],data=fulldata.in, family = "binomial")  # fit on all available data
    bestcoef = coef(bestfit)
  }
      #else the best model is one of knn, and theres no fitting since we just directly use it via prediction.
  # else {
    #bestfit = knn(trainx.out,validx.out,traindata.out$low, k = bestmodel.in-nbaseModels)
    #bestcoef = coef(bestfit) # coefficients for the best model fit - there are no coefficients in KNN (just k neigh)
  #}

  #########################################
  ##          Making predictions         ##
  ## (using best model/hyper parameters) ##
  #########################################  
   
    #using the empty vector, assign this iterations best model number so we can keep track of it.
  allbestmodels[j] = bestmodel.in
  
    #finally, perform the prediction process on this iterations "test" set using this iterations "best" model.
    #so for data points with indices of 1, use model 11 (which is a knn with k=3).
  if (bestmodel.in <= nbaseModels) {  # then best is one of linear models
    allpredictedCV.out[groupj.out] = ifelse(predict(bestfit,validdata.out, type = "response")>0.50,"2","1")
  } else {  
    allpredictedCV.out[groupj.out] = knn(trainx.out,validx.out,trainy.out, k = bestmodel.in - nbaseModels)
  }##################@@@@@!!!!!!!!!! Is this KNN setup properly? Using the training y's from the outter loop!!!!???
    ################################ Also, i need to re-standardize if im going to be using knn for prediction~~!!!!
  
  ####################################
  ##   End of CV outer loop         ##
  ## (will recycle for k.out times) ##
  ####################################  
}

allpredictedCV.out = ifelse(allpredictedCV.out=="1",0,1)
```

```{r}

#############################################################
##### Now 2x CV is done, lets assess the results        #####				 
#############################################################

# for curiosity, we can see the models that were "best" on each of the inner splits
allbestmodels

#assessment
table(allpredictedCV.out,fulldata.out$low)


(42+15) / (115+42+15+17)


library(pROC)
  #creating the roc objects.
best.roc = roc(response = birth$low, predictor = allpredictedCV.out)


  #plotting the roc curves
plot(best.roc)
auc(best.roc)


```






















