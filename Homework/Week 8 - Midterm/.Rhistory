knn.allmisclass[m+600] = 1-((cmat[1,1] + 0) / sum(cmat))
}
}
} else {
for(m in 1:k.neighbors){ #age+race+smoke+ptl+ht+ui+ftv+loglwt
cmat = table(knn.allpredictedCV[,m,model.index],birth$low)
if(dim(cmat)[1] == 2){
knn.allmisclass[m+700] = 1-((cmat[1,1] + cmat[2,2]) / sum(cmat))
}else{
knn.allmisclass[m+700] = 1-((cmat[1,1] + 0) / sum(cmat))
}
}
}
model.index = model.index + 1
}
#display min mis-class error rate and the associated model number.
which.min(knn.allmisclass); knn.allmisclass[which.min(knn.allmisclass)]
#visualize CV10 error across all k neighbors, and identify lowest point with red star.
plot(seq(1:(k.neighbors*nbaseModels)),knn.allmisclass, type = "l", xlab = "K-Neighbors", ylab = "Misclass Error Rate", main = "Misclassification Error Rates for KNN")
points(which.min(knn.allmisclass),knn.allmisclass[which.min(knn.allmisclass)], col = "red", pch = 8)
#read in dataset
birth = read.csv("C:/Users/joshj/Documents/DS740-Data Mining and Machine Learning/Homework/Week 8 - Midterm/birthwt.csv")
#check attribute data types.
str(birth)
#remove the "bwt" variable.
birth = birth[,-2]
#convert the following variables to factor type.
birth$low = as.factor(birth$low)
birth$race = as.factor(birth$race)
birth$smoke = as.factor(birth$smoke)
birth$ht = as.factor(birth$ht)
birth$ui = as.factor(birth$ui)
summary(birth)
library(ggformula)
library(ggpubr)
#create boxplots and histograms.
ggarrange(gf_boxplot(lwt~low, data=birth, color = ~low, xlab="", ylab="", title='Distribution of "lwt" variable'),
gf_boxplot(age~low, data=birth, color = ~low, xlab="", ylab="", title='Distribution of "age" variable'),
gf_histogram(~lwt, data = birth, color = ~low, fill = ~low, xlab = "", ylab = ""),
gf_histogram(~age, data = birth, color = ~low, fill = ~low, xlab = "", ylab = ""),
ncol = 2,nrow = 2)
#log transform lwt, and remove normal lwt.
birth$loglwt = log(birth$lwt)
birth = birth[,-3]
#checking correlation between terms
cor(birth[,c(2,5,8,9)])
#visualizing any relationships
plot(birth)
#setting models.
model1 = (low~age)
model2 = (low~age+race)
model3 = (low~age+race+smoke)
model4 = (low~age+race+smoke+ptl)
model5 = (low~age+race+smoke+ptl+ht)
model6 = (low~age+race+smoke+ptl+ht+ui)
model7 = (low~age+race+smoke+ptl+ht+ui+ftv)
model8 = (low~age+race+smoke+ptl+ht+ui+ftv+loglwt)
allModels = list(model1,model2,model3,model4,model5,model6,model7,model8)
#setting number of base candidate models
nbaseModels = length(allModels)
#sample size; setting 10-folds for CV.
n = nrow(birth)
k = 10
#randomize 10-fold groups.
groups = c(rep(1:k,floor(n/k)),1:(n-floor(n/k)*k))
set.seed(1234, sample.kind = "Rounding")
cvgroups = sample(groups,n)
library(class)
set.seed(1234, sample.kind = "Rounding")
#set number of neighbors to try
k.neighbors = 100
#create storage for all predicted values.
knn.allpredictedCV = array(rep(NA,n*k.neighbors*nbaseModels), dim = c(nrow(birth),k.neighbors,nbaseModels))
for(model.index in 1:nbaseModels){
for (i in 1:k) {
test = (cvgroups == i)
train.x = birth[!test,]
test.x = birth[test,]
#scale numeric test predictors.
age.train.std = scale(train.x$age); ptl.train.std = scale(train.x$ptl)
ftv.train.std = scale(train.x$ftv); loglwt.train.std = scale(train.x$loglwt)
#scale numeric train predictors
age.test.std = scale(test.x$age,
center = attr(age.train.std,"scaled:center"),
scale = attr(age.train.std,"scaled:scale"))
ptl.test.std = scale(test.x$ptl,
center = attr(ptl.train.std,"scaled:center"),
scale = attr(ptl.train.std,"scaled:scale"))
ftv.test.std = scale(test.x$ftv,
center = attr(ftv.train.std,"scaled:center"),
scale = attr(ftv.train.std,"scaled:scale"))
loglwt.test.std = scale(test.x$loglwt,
center = attr(loglwt.train.std,"scaled:center"),
scale = attr(loglwt.train.std,"scaled:scale"))
#depending on model iteration, select appropriate combination of predictors for train/test for knn.
if (model.index == 1){
train.x.std = data.frame(age.train.std)
test.x.std = data.frame(age.test.std)
} else if (model.index == 2){
train.x.std = data.frame(age.train.std,
train.x$race)
test.x.std = data.frame(age.test.std,
test.x$race)
} else if (model.index == 3){
train.x.std = data.frame(age.train.std,
train.x$race,
train.x$smoke)
test.x.std = data.frame(age.test.std,
test.x$race,
test.x$smoke)
} else if (model.index == 4){
train.x.std = data.frame(age.train.std,
train.x$race,
train.x$smoke,
ptl.train.std)
test.x.std = data.frame(age.test.std,
test.x$race,
test.x$smoke,
ptl.test.std)
} else if (model.index == 5){
train.x.std = data.frame(age.train.std,
train.x$race,
train.x$smoke,
ptl.train.std,
train.x$ht)
test.x.std = data.frame(age.test.std,
test.x$race,
test.x$smoke,
ptl.test.std,
test.x$ht)
} else if (model.index == 6){
train.x.std = data.frame(age.train.std,
train.x$race,
train.x$smoke,
ptl.train.std,
train.x$ht,
train.x$ui)
test.x.std = data.frame(age.test.std,
test.x$race,
test.x$smoke,
ptl.test.std,
test.x$ht,
test.x$ui)
} else if (model.index == 7){
train.x.std = data.frame(age.train.std,
train.x$race,
train.x$smoke,
ptl.train.std,
train.x$ht,
train.x$ui,
ftv.train.std)
test.x.std = data.frame(age.test.std,
test.x$race,
test.x$smoke,
ptl.test.std,
test.x$ht,
test.x$ui,
ftv.test.std)
} else {
train.x.std = data.frame(age.train.std,
train.x$race,
train.x$smoke,
ptl.train.std,
train.x$ht,
train.x$ui,
ftv.train.std,
loglwt.train.std)
test.x.std = data.frame(age.test.std,
test.x$race,
test.x$smoke,
ptl.test.std,
test.x$ht,
test.x$ui,
ftv.test.std,
loglwt.test.std)
}
#perform knn - 100x for each fold, for current model.
for (K in 1:k.neighbors) {
knn.allpredictedCV[test,K,model.index] = knn(train.x,test.x,birth$low[!test], k = K)
}
}
model.index = model.index + 1
}
#reset model index for misclassification calculation.
model.index = 1
#create a vector to store classification error rates.
knn.allmisclass = rep(-1,k.neighbors*nbaseModels)
#calculate mis-class error rates for each of the 800 models
for (model.index in 1:nbaseModels){
if (model.index == 1){  #age
for(m in 1:k.neighbors){
cmat = table(knn.allpredictedCV[,m,model.index],birth$low)
if(dim(cmat)[1] == 2){
knn.allmisclass[m] = 1-((cmat[1,1] + cmat[2,2]) / sum(cmat))
}else{
knn.allmisclass[m] = 1-((cmat[1,1] + 0) / sum(cmat))
}
}
} else if (model.index == 2){ #age+race
for(m in 1:k.neighbors){
cmat = table(knn.allpredictedCV[,m,model.index],birth$low)
if(dim(cmat)[1] == 2){
knn.allmisclass[m+100] = 1-((cmat[1,1] + cmat[2,2]) / sum(cmat))
}else{
knn.allmisclass[m+100] = 1-((cmat[1,1] + 0) / sum(cmat))
}
}
} else if (model.index == 3){ #age+race+smoke
for(m in 1:k.neighbors){
cmat = table(knn.allpredictedCV[,m,model.index],birth$low)
if(dim(cmat)[1] == 2){
knn.allmisclass[m+200] = 1-((cmat[1,1] + cmat[2,2]) / sum(cmat))
}else{
knn.allmisclass[m+200] = 1-((cmat[1,1] + 0) / sum(cmat))
}
}
} else if (model.index == 4){ #age+race+smoke+ptl
for(m in 1:k.neighbors){
cmat = table(knn.allpredictedCV[,m,model.index],birth$low)
if(dim(cmat)[1] == 2){
knn.allmisclass[m+300] = 1-((cmat[1,1] + cmat[2,2]) / sum(cmat))
}else{
knn.allmisclass[m+300] = 1-((cmat[1,1] + 0) / sum(cmat))
}
}
} else if (model.index == 5){ #age+race+smoke+ptl+ht
for(m in 1:k.neighbors){
cmat = table(knn.allpredictedCV[,m,model.index],birth$low)
if(dim(cmat)[1] == 2){
knn.allmisclass[m+400] = 1-((cmat[1,1] + cmat[2,2]) / sum(cmat))
}else{
knn.allmisclass[m+400] = 1-((cmat[1,1] + 0) / sum(cmat))
}
}
} else if (model.index == 6){ #age+race+smoke+ptl+ht+ui
for(m in 1:k.neighbors){
cmat = table(knn.allpredictedCV[,m,model.index],birth$low)
if(dim(cmat)[1] == 2){
knn.allmisclass[m+500] = 1-((cmat[1,1] + cmat[2,2]) / sum(cmat))
}else{
knn.allmisclass[m+500] = 1-((cmat[1,1] + 0) / sum(cmat))
}
}
} else if (model.index == 7){ #age+race+smoke+ptl+ht+ui+ftv
for(m in 1:k.neighbors){
cmat = table(knn.allpredictedCV[,m,model.index],birth$low)
if(dim(cmat)[1] == 2){
knn.allmisclass[m+600] = 1-((cmat[1,1] + cmat[2,2]) / sum(cmat))
}else{
knn.allmisclass[m+600] = 1-((cmat[1,1] + 0) / sum(cmat))
}
}
} else {
for(m in 1:k.neighbors){ #age+race+smoke+ptl+ht+ui+ftv+loglwt
cmat = table(knn.allpredictedCV[,m,model.index],birth$low)
if(dim(cmat)[1] == 2){
knn.allmisclass[m+700] = 1-((cmat[1,1] + cmat[2,2]) / sum(cmat))
}else{
knn.allmisclass[m+700] = 1-((cmat[1,1] + 0) / sum(cmat))
}
}
}
model.index = model.index + 1
}
#display min mis-class error rate and the associated model number.
which.min(knn.allmisclass); knn.allmisclass[which.min(knn.allmisclass)]
#visualize CV10 error across all k neighbors, and identify lowest point with red star.
plot(seq(1:(k.neighbors*nbaseModels)),knn.allmisclass, type = "l", xlab = "K-Neighbors", ylab = "Misclass Error Rate", main = "Misclassification Error Rates for KNN")
points(which.min(knn.allmisclass),knn.allmisclass[which.min(knn.allmisclass)], col = "red", pch = 8)
library(randomForest)
set.seed(1234, sample.kind = "Rounding")
#create storage for 171 predictions for 36 random forest models.
rf.allpredictedCV = matrix(rep(NA,n*36),ncol=36)
#capture predictions performing CV10 for 36 models and store in matrix.
for (i in 1:k)  {
train = (cvgroups != i)
test = (cvgroups == i)
index = 1
for (m in 1:nbaseModels) {
for (l in 1:m) {
rf.fit = randomForest(formula = allModels[[m]], data = birth[train,], mtry = l)
rf.allpredictedCV[test,index] = predict(rf.fit, newdata = birth[test,], type = "prob")[,2]
index = index +1
}
}
}
#create a vector to store classification error rates
rf.allmisclass = rep(-1,36)
#calculate mis-class error rates for each of the 36 models.
for(m in 1:36){
cmat = table(ifelse(rf.allpredictedCV[,m]>0.50,"1","0"),birth$low)
if(dim(cmat)[1] == 2){
rf.allmisclass[m] = 1-((cmat[1,1] + cmat[2,2]) / sum(cmat))
}else{
rf.allmisclass[m] = 1-((cmat[1,1] + 0) / sum(cmat))
}
}
#display min mis-class error rate and the associated model number.
which.min(rf.allmisclass); rf.allmisclass[which.min(rf.allmisclass)]
#visualize CV10 error across all random forest models, and identify lowest point with red star.
plot(seq(1:36),rf.allmisclass, type = "l", xlab = "Model Number", ylab = "Misclass Error Rate", main = "Misclassification Error Rates for Random Forest")
points(which.min(rf.allmisclass),rf.allmisclass[which.min(rf.allmisclass)], col = "red", pch = 8)
################################################################
##  Double cross-validation for modeling assessment           ##
################################################################
#############################################
##    General prep for OUTER CV process    ##
#############################################
#prepare parameters and randomize the indices for CV10
fulldata.out = birth
k.out = 10
n.out = nrow(fulldata.out)
groups.out = c(rep(1:k.out,floor(n.out/k.out)))
if(floor(n.out/k.out) != (n.out/k.out)){
groups.out = c(groups.out, 1:(n.out%%k.out))
}
set.seed(1234, sample.kind = "Rounding")
cvgroups.out = sample(groups.out,n.out)
#create storage for all CV scores of outer CV, and storage for best model that was used.
allpredictedCV.out = rep(NA,n.out)
allbestmodels = rep(NA,k.out)
####################################
###   Begin double-CV process    ###
####################################
for (j in 1:k.out){
################################################################################
##                   Split data for OUTER CV10 process                        ##
################################################################################
#identify indices for outer CV10 groups.
groupj.out = (cvgroups.out == j)
#split full data into train/test folds.
traindata.out = fulldata.out[!groupj.out,]
testdata.out = fulldata.out[groupj.out,]
#scale numeric test.out predictors.
std.age.train.out = scale(traindata.out$age); std.ptl.train.out = scale(traindata.out$ptl);
#scale numeric train.out predictors
std.age.test.out = scale(testdata.out$age,
center = attr(std.age.train.out,"scaled:center"),
scale = attr(std.age.train.out,"scaled:scale"))
std.ptl.test.out = scale(testdata.out$ptl,
center = attr(std.ptl.train.out,"scaled:center"),
scale = attr(std.ptl.train.out,"scaled:scale"))
#rebuild training.out and testing.out dataframes for knn.
std.train.x.out = data.frame(std.age.train.out,
traindata.out$race,
traindata.out$smoke,
std.ptl.train.out)
std.test.x.out = data.frame(std.age.test.out,
testdata.out$race,
testdata.out$smoke,
std.ptl.test.out)
####################################
### pipe current training data   ###
####################################
########	:	:	:	:	:	:	:  ###########
#pass train splits from outer CV loop in as "full" dataset for inner CV loop.
fulldata.in = traindata.out
########	:	:	:	:	:	:	:  ###########
####################################
####  start INNER CV process    ####
####################################
################################################################################
##                    ...Prep for INNER CV process...                         ##
################################################################################
#setup inner CV parameters, and prep for splitting fulldata.in (which is traindata.out).
k.in = 10
n.in = nrow(fulldata.in)
groups.in = c(rep(1:k.in,floor(n.in/k.in)))
if(floor(n.in/k.in) != (n.in/k.in)){
groups.in = c(groups.in, 1:(n.in%%k.in))
}
cvgroups.in = sample(groups.in,n.in)
#############################################
### start INNER CV for random forest       ##
#############################################
#create storage for 36 random forest model predictions
rf.allpredictedCV.in = matrix(rep(NA,n.in*36),ncol=36)
#capture predictions performing CV10 for 36 models and store in matrix.
for (i in 1:k.in)  {
train.in = (cvgroups.in != i)
test.in = (cvgroups.in == i)
index = 1
for (m in 1:nbaseModels) {
for (l in 1:m) {
rf.fit = randomForest(formula = allModels[[m]], data = fulldata.in[train.in,], mtry = l)
rf.allpredictedCV.in[test.in,index] = predict(rf.fit, newdata = fulldata.in[test.in,], type = "prob")[,2]
index = index +1
}
}
}
#create a vector to store classification error rates
rf.allmisclass.in = rep(-1,36)
#calculate mis-class error rates for each of the 36 models.
for(m in 1:36){
cmat.in = table(ifelse(rf.allpredictedCV.in[,m]>0.50,"1","0"),fulldata.in$low)
if(dim(cmat.in)[1] == 2){
rf.allmisclass.in[m] = 1-((cmat.in[1,1] + cmat.in[2,2]) / sum(cmat.in))
}else{
rf.allmisclass.in[m] = 1-((cmat.in[1,1] + 0) / sum(cmat.in))
}
}
#############################################
##   end INNER CV for random forest/       ##
## /start INNER CV for k-nearest neighbors ##
#############################################
#create storage for 170 predictions for 100 k-neighbor models.
knn.allpredictedCV.in = matrix(rep(NA,n.in*k.neighbors),ncol=k.neighbors)
#capture predictions performing CV10 for 100 models and store in matrix.
for (i in 1:k.in)  {
test.in = (cvgroups.in == i)
train.x.in = fulldata.in[!test.in,]
test.x.in = fulldata.in[test.in,]
#scale numeric test.in predictors.
std.age.train.in = scale(train.x.in$age); std.ptl.train.in = scale(train.x.in$ptl)
#scale numeric train.in predictors
std.age.test.in = scale(test.x.in$age,
center = attr(std.age.train.in,"scaled:center"),
scale = attr(std.age.train.in,"scaled:scale"))
std.ptl.test.in = scale(test.x.in$ptl,
center = attr(std.ptl.train.in,"scaled:center"),
scale = attr(std.ptl.train.in,"scaled:scale"))
#rebuild training.in and testing.in dataframes for knn.
std.train.x.in = data.frame(std.age.train.in,
train.x.in$race,
train.x.in$smoke,
std.ptl.train.in)
std.test.x.in = data.frame(std.age.test.in,
test.x.in$race,
test.x.in$smoke,
std.ptl.test.in)
for (m in 1:k.neighbors) {
knn.allpredictedCV.in[test.in,m] = knn(std.train.x.in,std.test.x.in,fulldata.in$low[!test.in], k = m)
}
}
#create storage for 100 miss-class error rates.
knn.allmisclass.in = rep(-1,k.neighbors)
#calculate miss-class error rates for each of the 100 models.
for(m in 1:k.neighbors){
cmat.in = table(knn.allpredictedCV.in[,m],fulldata.in$low)
if(dim(cmat.in)[1] == 2){
knn.allmisclass.in[m] = 1-((cmat.in[1,1] + cmat.in[2,2]) / sum(cmat.in))
}else{
knn.allmisclass.in[m] = 1-((cmat.in[1,1] + 0) / sum(cmat.in))
}
}
############################################
## end INNER CV for k-nearest neighbors   ##
############################################
#############################################
##  end INNER CV calc for both models/     ##
## /Start model fitting OUTER CV loop      ##
#############################################
#############################################
##        identify best model              ##
#############################################
#combining CV10 error rates, and selecting index of lowest rate - that is best model.
bestmodel.in = order(c(rf.allmisclass.in,knn.allmisclass.in))[1]
############################################
##        fit best model                  ##
############################################
#now that we have identified best model, fit it to the outer CV10's training data.
#if the model was a random forest, then we need to fit that with the right number of predictors and mtry.
#Note: if the best model was a KNN, then we go right into prediction step.
if (bestmodel.in <= 36) { # then best is one of random forest models
if(bestmodel.in == 1){        #RF 1 predictor
bestfit = randomForest(formula = allModels[[bestmodel.in]], data = traindata.out, mtry = bestmodel.in)
} else if(bestmodel.in <=3){ #RF 2 predictor
bestfit = randomForest(formula = allModels[[ceiling(bestmodel.in/nbaseModels)]], data = traindata.out,
mtry = bestmodel.in-1)
} else if(bestmodel.in <=6){ #RF 3 predictor
bestfit = randomForest(formula = allModels[[ceiling(bestmodel.in/nbaseModels)]], data = traindata.out,
mtry = bestmodel.in-3)
} else if(bestmodel.in <=10){ #RF 4 predictor
bestfit = randomForest(formula = allModels[[ceiling(bestmodel.in/nbaseModels)]], data = traindata.out,
mtry = bestmodel.in-6)
} else if(bestmodel.in <=15){ #RF 5 predictor
bestfit = randomForest(formula = allModels[[ceiling(bestmodel.in/nbaseModels)]], data = traindata.out,
mtry = bestmodel.in-10)
} else if(bestmodel.in <=21){ #RF 6 predictor
bestfit = randomForest(formula = allModels[[ceiling(bestmodel.in/nbaseModels)]], data = traindata.out,
mtry = bestmodel.in-15)
} else if(bestmodel.in <=28){ #RF 7 predictor
bestfit = randomForest(formula = allModels[[ceiling(bestmodel.in/nbaseModels)]], data = traindata.out,
mtry = bestmodel.in-21)
} else                      { #RF 8 predictor
bestfit = randomForest(formula = allModels[[ceiling(bestmodel.in/nbaseModels)]], data = traindata.out,
mtry = bestmodel.in-28)
}
}
######################################################################
##  Making predictions w/best model on the "outer" holdout sample   ##
######################################################################
#keep track of best 2x CV models for curiosity.
allbestmodels[j] = bestmodel.in
#preform prediction using the outer CV10's test/validation set. Store in vector.
#if model was 36 or less, use the model fitted with random forest.
#else predict response using knn.
if (bestmodel.in <= 36) {  # then best is one of random forest
allpredictedCV.out[groupj.out] = ifelse(predict(bestfit,newdata = testdata.out, type = "prob")[,2]>0.50,2,1)
} else {
allpredictedCV.out[groupj.out] = knn(std.train.x.out,std.test.x.out,fulldata.out$low[!groupj.out], k = bestmodel.in - 36)
}
####################################
##   End of CV outer loop         ##
## (will recycle for k.out times) ##
####################################
}
#match the output of knn with the actual response classes.
allpredictedCV.out = ifelse(allpredictedCV.out=="1",0,1)
#create confusion matrix and calculate error rate.
cmat.best = table(allpredictedCV.out,birth$low); cmat.best
1-((cmat.best[1,1] + cmat.best[2,2]) / sum(cmat.best))
