---
title: "Mid-term Low Birthweight Prediction"
author: "Josh Jarvey"
date: "10/26/2020"
output: word_document
---

## 1.0 - Data preparation:

First we will read in our dataset, and check our variable types.

```{r}
  #read in dataset
birth = read.csv("C:/Users/joshj/Documents/DS740-Data Mining and Machine Learning/Homework/Week 8 - Midterm/birthwt.csv")
  #check attribute data types. 
str(birth)
```

Since we will be modeling baby's birth weight lower than 2.5kg (low) variable, we will remove the redundant "bwt" quantitative variable.

```{r}
  #remove the "bwt" variable.
birth = birth[,-2]
```

Some variables will need to be converted to the proper types for analysis.

```{r}
  #convert the following variables to factor type.
birth$low = as.factor(birth$low)
birth$race = as.factor(birth$race)
birth$smoke = as.factor(birth$smoke)
birth$ht = as.factor(birth$ht)
birth$ui = as.factor(birth$ui)
```

Now that we have the variables in the proper format, lets take a look at the summary to see if we have any data that's missing, mis-coded, or extreme outliers that would warrant further investigation.

```{r}
summary(birth)
```

We can also look at histograms/boxplots of the quantitative variables to identify if there are any skews to the observations. 

```{r,message=FALSE, echo=FALSE, fig.width = 10, fig.height = 6}
library(ggformula)
library(ggpubr)
  #create boxplots and histograms. 
ggarrange(gf_boxplot(lwt~low, data=birth, color = ~low, xlab="", ylab="", title='Distribution of "lwt" variable'),
          gf_boxplot(age~low, data=birth, color = ~low, xlab="", ylab="", title='Distribution of "age" variable'),
          gf_histogram(~lwt, data = birth, color = ~low, fill = ~low, xlab = "", ylab = ""),
          gf_histogram(~age, data = birth, color = ~low, fill = ~low, xlab = "", ylab = ""),
          ncol = 2,nrow = 2)
```
There does appear to be some skew in the mother's weight variable "lwt", so before we move forward with fitting a model we will perform a log() transformation on the variable, and we'll remove the original from the data set. 

```{r}
  #log transform lwt, and remove normal lwt.
birth$loglwt = log(birth$lwt)
birth = birth[,-3]
```

As a final step, we'll check for any signs of collinearity by checking correlations among predictor variables and generate scatterplots to visualize any relationships.

```{r}
  #checking correlation between terms
cor(birth[,c(2,5,8,9)])
  #visualizing any relationships
plot(birth)
```

There are no strong correlations among the set of predictors, and we can move onto our modeling selection process.

## 2.0 - MODELING SELECTION PROCESS:

Now that our data is prepared, we can begin the modeling process. Since our variable of interest is a binary response, we will investigate methods that result in a binary prediction. Because we intend to do predictor selection and cross-validation, we will create the base models and set up 10-fold splits:

```{r}
  #setting models.
model1 = (low~age)
model2 = (low~age+race)
model3 = (low~age+race+smoke)
model4 = (low~age+race+smoke+ptl)
model5 = (low~age+race+smoke+ptl+ht)
model6 = (low~age+race+smoke+ptl+ht+ui)
model7 = (low~age+race+smoke+ptl+ht+ui+ftv)
model8 = (low~age+race+smoke+ptl+ht+ui+ftv+loglwt)
allModels = list(model1,model2,model3,model4,model5,model6,model7,model8)
  #setting number of base candidate models
nbaseModels = length(allModels)

  #sample size; setting 10-folds for CV.
n = nrow(birth)
k = 10
  #randomize 10-fold groups.
groups = c(rep(1:k,floor(n/k)),1:(n-floor(n/k)*k)) 
set.seed(8, sample.kind = "Rounding") 
cvgroups = sample(groups,n)
```

## 2.1 - Logistic Regression:

For our first candidate model type, we will plan to fit a logistic regression since this is a well known method. First we must check our assumptions about the logistic regression model - this shows logistic regression wont be viable due to logit not linear with AGE variable. 

```{r}
#this code was selected and modified from the online article:
#http://www.sthda.com/english/articles/36-classification-methods-essentials/148-logistic-regression-assumptions-and-diagnostics-in-r/ 
library(tidyverse)
library(broom)

#1: checking that continuous variables are linear to the logit of the response.
  #full model fit to generate logit values
fullfit = glm(low~., data = birth, family = "binomial")
probabilities = predict(fullfit, type = "response")
predicted.classes = ifelse(probabilities>0.5,1,0)
  #only check continuous variables
mydata = birth[,c(2,9)]
  #build a stacked dataframe with the logit calculation as a column.
mydata = mydata %>%
  mutate(logit = log(probabilities/(1-probabilities))) %>%
  gather(key = "predictors", value = "predictor.value", -logit)
  #build the scatterplot and display a loess line to check for linearity.
ggplot(mydata, aes(logit, predictor.value))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "loess") + 
  theme_bw() + 
  facet_wrap(~predictors, scales = "free_y")

#2: Check for influential outliers
  #check points on a plot of cooks distance
plot(fullfit, which = 4, id.n = 3)
  #plot residuals
model.data <- augment(fullfit) %>% 
  mutate(index = 1:n()) 
ggplot(model.data, aes(index, .std.resid)) + 
  geom_point(aes(color = low), alpha = .5) +
  theme_bw()
  #check if any residuals are greater than 3 (none, so this checks)
model.data %>% 
  filter(abs(.std.resid) > 3)

#3: Check for collinearity. None are above a score of 10.
library(car)
vif(fullfit)

#4: response is binary. OK.
```

## 2.2 - K-Nearest Neighbor (knn):

Because the predictors aren't very linear in relationship, we opt to use the k-nearest neighbors method. Here we set up 10-fold cross-validation, and within each fold we scale our quantitative predictors, and try out numerous values for k-neighbors (100) tuning parameter.

```{r}
library(class)
set.seed(8, sample.kind = "Rounding") 
  #set number of neighbors to try
k.neighbors = 100
  #create storage for all predicted values.
knn.allpredictedCV = array(rep(NA,n*k.neighbors*nbaseModels), dim = c(nrow(birth),k.neighbors,nbaseModels))

for(model.index in 1:nbaseModels){
  for (i in 1:k) {
    test = (cvgroups == i)
    train.x = birth[!test,]
    test.x = birth[test,]
  
        #scale numeric test predictors.
    age.train.std = scale(train.x$age); ptl.train.std = scale(train.x$ptl)
    ftv.train.std = scale(train.x$ftv); loglwt.train.std = scale(train.x$loglwt)
    
      #scale numeric train predictors
    age.test.std = scale(test.x$age, 
                           center = attr(age.train.std,"scaled:center"), 
                           scale = attr(age.train.std,"scaled:scale"))
    ptl.test.std = scale(test.x$ptl, 
                           center = attr(ptl.train.std,"scaled:center"), 
                           scale = attr(ptl.train.std,"scaled:scale"))
    ftv.test.std = scale(test.x$ftv, 
                           center = attr(ftv.train.std,"scaled:center"), 
                           scale = attr(ftv.train.std,"scaled:scale"))
    loglwt.test.std = scale(test.x$loglwt, 
                           center = attr(loglwt.train.std,"scaled:center"), 
                           scale = attr(loglwt.train.std,"scaled:scale"))

      #depending on model iteration, select appropriate combination of predictors for train/test for knn.
    if (model.index == 1){
      train.x.std = data.frame(age.train.std)
      test.x.std = data.frame(age.test.std)      
    } else if (model.index == 2){
      train.x.std = data.frame(age.train.std,
                               train.x$race)
      test.x.std = data.frame(age.test.std,
                              test.x$race) 
    } else if (model.index == 3){
      train.x.std = data.frame(age.train.std,
                               train.x$race,
                               train.x$smoke)
      test.x.std = data.frame(age.test.std,
                              test.x$race,
                              test.x$smoke) 
    } else if (model.index == 4){
      train.x.std = data.frame(age.train.std,
                               train.x$race,
                               train.x$smoke,
                               ptl.train.std)
      test.x.std = data.frame(age.test.std,
                              test.x$race,
                              test.x$smoke,
                              ptl.test.std) 
    } else if (model.index == 5){
      train.x.std = data.frame(age.train.std,
                               train.x$race,
                               train.x$smoke,
                               ptl.train.std,
                               train.x$ht)
      test.x.std = data.frame(age.test.std,
                              test.x$race,
                              test.x$smoke,
                              ptl.test.std,
                              test.x$ht) 
    } else if (model.index == 6){
      train.x.std = data.frame(age.train.std,
                               train.x$race,
                               train.x$smoke,
                               ptl.train.std,
                               train.x$ht,
                               train.x$ui)
      test.x.std = data.frame(age.test.std,
                              test.x$race,
                              test.x$smoke,
                              ptl.test.std,
                              test.x$ht,
                              test.x$ui) 
    } else if (model.index == 7){
      train.x.std = data.frame(age.train.std,
                               train.x$race,
                               train.x$smoke,
                               ptl.train.std,
                               train.x$ht,
                               train.x$ui,
                               ftv.train.std)
      test.x.std = data.frame(age.test.std,
                              test.x$race,
                              test.x$smoke,
                              ptl.test.std,
                              test.x$ht,
                              test.x$ui,
                              ftv.test.std) 
    } else {
      train.x.std = data.frame(age.train.std,
                               train.x$race,
                               train.x$smoke,
                               ptl.train.std,
                               train.x$ht,
                               train.x$ui,
                               ftv.train.std,
                               loglwt.train.std)
      test.x.std = data.frame(age.test.std,
                              test.x$race,
                              test.x$smoke,
                              ptl.test.std,
                              test.x$ht,
                              test.x$ui,
                              ftv.test.std,
                              loglwt.test.std) 
    }
      #perform knn - 100x for each fold, for current model.
    for (K in 1:k.neighbors) {
      knn.allpredictedCV[test,K,model.index] = knn(train.x,test.x,birth$low[!test], k = K)
    }
  }
  model.index = model.index + 1
}

  #reset model index for misclassification calculation.
model.index = 1
  #create a vector to store classification error rates.
knn.allmisclass = rep(-1,k.neighbors*nbaseModels) 
  #calculate mis-class error rates for each of the 800 models
for (model.index in 1:nbaseModels){
  if (model.index == 1){  #age
    for(m in 1:k.neighbors){
      cmat = table(knn.allpredictedCV[,m,model.index],birth$low)
      if(dim(cmat)[1] == 2){
        knn.allmisclass[m] = 1-((cmat[1,1] + cmat[2,2]) / sum(cmat))
      }else{
        knn.allmisclass[m] = 1-((cmat[1,1] + 0) / sum(cmat))
      }
    }
  } else if (model.index == 2){ #age+race
    for(m in 1:k.neighbors){
      cmat = table(knn.allpredictedCV[,m,model.index],birth$low)
      if(dim(cmat)[1] == 2){
        knn.allmisclass[m+100] = 1-((cmat[1,1] + cmat[2,2]) / sum(cmat))
      }else{
        knn.allmisclass[m+100] = 1-((cmat[1,1] + 0) / sum(cmat))
      }
    }    
  } else if (model.index == 3){ #age+race+smoke
    for(m in 1:k.neighbors){
      cmat = table(knn.allpredictedCV[,m,model.index],birth$low)
      if(dim(cmat)[1] == 2){
        knn.allmisclass[m+200] = 1-((cmat[1,1] + cmat[2,2]) / sum(cmat))
      }else{
        knn.allmisclass[m+200] = 1-((cmat[1,1] + 0) / sum(cmat))
      }
    }    
  } else if (model.index == 4){ #age+race+smoke+ptl
    for(m in 1:k.neighbors){
      cmat = table(knn.allpredictedCV[,m,model.index],birth$low)
      if(dim(cmat)[1] == 2){
        knn.allmisclass[m+300] = 1-((cmat[1,1] + cmat[2,2]) / sum(cmat))
      }else{
        knn.allmisclass[m+300] = 1-((cmat[1,1] + 0) / sum(cmat))
      }
    }    
  } else if (model.index == 5){ #age+race+smoke+ptl+ht
    for(m in 1:k.neighbors){
      cmat = table(knn.allpredictedCV[,m,model.index],birth$low)
      if(dim(cmat)[1] == 2){
        knn.allmisclass[m+400] = 1-((cmat[1,1] + cmat[2,2]) / sum(cmat))
      }else{
        knn.allmisclass[m+400] = 1-((cmat[1,1] + 0) / sum(cmat))
      }
    }    
  } else if (model.index == 6){ #age+race+smoke+ptl+ht+ui
    for(m in 1:k.neighbors){
      cmat = table(knn.allpredictedCV[,m,model.index],birth$low)
      if(dim(cmat)[1] == 2){
        knn.allmisclass[m+500] = 1-((cmat[1,1] + cmat[2,2]) / sum(cmat))
      }else{
        knn.allmisclass[m+500] = 1-((cmat[1,1] + 0) / sum(cmat))
      }
    }    
  } else if (model.index == 7){ #age+race+smoke+ptl+ht+ui+ftv
    for(m in 1:k.neighbors){
      cmat = table(knn.allpredictedCV[,m,model.index],birth$low)
      if(dim(cmat)[1] == 2){
        knn.allmisclass[m+600] = 1-((cmat[1,1] + cmat[2,2]) / sum(cmat))
      }else{
        knn.allmisclass[m+600] = 1-((cmat[1,1] + 0) / sum(cmat))
      }
    }    
  } else {
    for(m in 1:k.neighbors){ #age+race+smoke+ptl+ht+ui+ftv+loglwt
      cmat = table(knn.allpredictedCV[,m,model.index],birth$low)
      if(dim(cmat)[1] == 2){
        knn.allmisclass[m+700] = 1-((cmat[1,1] + cmat[2,2]) / sum(cmat))
      }else{
        knn.allmisclass[m+700] = 1-((cmat[1,1] + 0) / sum(cmat))
      }
    }    
  }
  model.index = model.index + 1
}
  #display min mis-class error rate and the associated model number.
which.min(knn.allmisclass); knn.allmisclass[which.min(knn.allmisclass)]
  #visualize CV10 error across all k neighbors, and identify lowest point with red star.
plot(seq(1:(k.neighbors*nbaseModels)),knn.allmisclass, type = "l", xlab = "Models", ylab = "Misclass Error Rate", main = "Multiple Predictors and K-Neighbor's Error Rates")
points(which.min(knn.allmisclass),knn.allmisclass[which.min(knn.allmisclass)], col = "red", pch = 8)
```

Overall, we discover that a K=1 with 7 predictors (age+race+smoke+ptl+ht+ui+ftv) produces a CV10 misclassification error rate of 14.28%.

## 2.3 - Random Forest

We choose random forest as our second method since it also can work with non-linear predictors. Also, it is more robust against outliers/influencers since it always picks a random subset of observations to build each tree in the random forest.  

```{r}
library(randomForest)
set.seed(8, sample.kind = "Rounding") 
  #create storage for 171 predictions for 36 random forest models.
rf.allpredictedCV = matrix(rep(NA,n*36),ncol=36)
  
  #capture predictions performing CV10 for 36 models and store in matrix.
for (i in 1:k)  {
  train = (cvgroups != i)
  test = (cvgroups == i)
  index = 1
  for (m in 1:nbaseModels) {
    for (l in 1:m) {
      rf.fit = randomForest(formula = allModels[[m]], data = birth[train,], mtry = l)
      rf.allpredictedCV[test,index] = predict(rf.fit, newdata = birth[test,], type = "prob")[,2]
      index = index +1
    }
  }
}
  #create a vector to store classification error rates
rf.allmisclass = rep(-1,36) 
  #calculate mis-class error rates for each of the 36 models.
for(m in 1:36){
  cmat = table(ifelse(rf.allpredictedCV[,m]>0.50,"1","0"),birth$low)
  if(dim(cmat)[1] == 2){
    rf.allmisclass[m] = 1-((cmat[1,1] + cmat[2,2]) / sum(cmat))
  }else{
    rf.allmisclass[m] = 1-((cmat[1,1] + 0) / sum(cmat))
  }
}
  #display min mis-class error rate and the associated model number.
which.min(rf.allmisclass); rf.allmisclass[which.min(rf.allmisclass)]
  #visualize CV10 error across all random forest models, and identify lowest point with red star.
plot(seq(1:36),rf.allmisclass, type = "l", xlab = "Models", ylab = "Misclass Error Rate", main = "Multiple Predictors and Mtry value's Error Rates")
points(which.min(rf.allmisclass),rf.allmisclass[which.min(rf.allmisclass)], col = "red", pch = 8)
```

Overall, we discover that a random forest with 5 predictors (age+race+smoke+ptl+ht), and an mtry=1, produces the best fit with a CV10 misclassification error rate of 29.10%.

Upon comparison of both KNN and Random forest, we find that KNN (k=1) with 7 predictors (age+race+smoke+ptl+ht+ui+ftv) is the superior model; misclassification error rate of 14.28% vs. 29.10%.

## 3.0 - MODEL ASSESSMENT FOR HONEST PREDICTION

As a final step in this analysis, we will preform double 10-fold cross-validation to assess an honest expectation of error rate for this model. We first split the enter dataset into 10 folds. One is held as an "outer" test, and the other 9 folds are sent into an "inner" modeling selection process where each of these models are also being fit and tested using a CV10 process. The best model from the "inner" fitting process is selected and used to predict the fold that was held back from the "outer" split. The predicted classes are stored, and once the "outer" CV10 process is complete, a confusion matrix is created and the overall misclassification rate is calculated.

Results - after the double cross-validation, the honest misclassification rate of the knn model is: *29.62%*

```{r}
################################################################
##  Double cross-validation for modeling assessment           ##				 
################################################################

#############################################
##    General prep for OUTER CV process    ##
#############################################
  #prepare parameters and randomize the indices for CV10
fulldata.out = birth
k.out = 10 
n.out = nrow(fulldata.out)
groups.out = c(rep(1:k.out,floor(n.out/k.out)))
if(floor(n.out/k.out) != (n.out/k.out)){
  groups.out = c(groups.out, 1:(n.out%%k.out))
}
set.seed(8, sample.kind = "Rounding") 
cvgroups.out = sample(groups.out,n.out)

  #create storage for all CV scores of outer CV, and storage for best model that was used.
allpredictedCV.out = rep(NA,n.out)
allbestmodels = rep(NA,k.out)
####################################
###   Begin double-CV process    ###
####################################
for (j in 1:k.out){ 

  ################################################################################
  ##                   Split data for OUTER CV10 process                        ##
  ################################################################################
    #identify indices for outer CV10 groups.
  groupj.out = (cvgroups.out == j)
  
    #split full data into train/test folds.
  traindata.out = fulldata.out[!groupj.out,]
  testdata.out = fulldata.out[groupj.out,]

    #scale numeric test.out predictors.
  std.age.train.out = scale(traindata.out$age); std.ptl.train.out = scale(traindata.out$ptl);
  std.ftv.train.out = scale(traindata.out$ftv)
    #scale numeric train.out predictors
  std.age.test.out = scale(testdata.out$age, 
                         center = attr(std.age.train.out,"scaled:center"), 
                         scale = attr(std.age.train.out,"scaled:scale"))
  std.ptl.test.out = scale(testdata.out$ptl, 
                         center = attr(std.ptl.train.out,"scaled:center"), 
                         scale = attr(std.ptl.train.out,"scaled:scale"))
  std.ftv.test.out = scale(testdata.out$ftv, 
                         center = attr(std.ftv.train.out,"scaled:center"), 
                         scale = attr(std.ftv.train.out,"scaled:scale"))
  
    #rebuild training.out and testing.out dataframes for knn.
  std.train.x.out = data.frame(std.age.train.out,
                           traindata.out$race,
                           traindata.out$smoke,
                           std.ptl.train.out,
                           traindata.out$ht,
                           traindata.out$ui,
                           std.ftv.train.out)
  std.test.x.out = data.frame(std.age.test.out,
                           testdata.out$race,
                           testdata.out$smoke,
                           std.ptl.test.out,
                           testdata.out$ht,
                           testdata.out$ui,
                           std.ftv.test.out)
  ####################################
  ### pipe current training data   ###
  ####################################
  ########	:	:	:	:	:	:	:  ###########
    #pass train splits from outer CV loop in as "full" dataset for inner CV loop. 
  fulldata.in = traindata.out
  ########	:	:	:	:	:	:	:  ###########
  ####################################
  ####  start INNER CV process    ####
  ####################################
  
  ################################################################################
  ##                    ...Prep for INNER CV process...                         ##
  ################################################################################
    #setup inner CV parameters, and prep for splitting fulldata.in (which is traindata.out).
  k.in = 10   
  n.in = nrow(fulldata.in)
  groups.in = c(rep(1:k.in,floor(n.in/k.in))) 
  if(floor(n.in/k.in) != (n.in/k.in)){
    groups.in = c(groups.in, 1:(n.in%%k.in))  
  }
  cvgroups.in = sample(groups.in,n.in)
  
  #############################################
  ### start INNER CV for random forest       ##
  #############################################
    #create storage for 36 random forest model predictions
  rf.allpredictedCV.in = matrix(rep(NA,n.in*36),ncol=36)
    
    #capture predictions performing CV10 for 36 models and store in matrix.
  for (i in 1:k.in)  {
    train.in = (cvgroups.in != i)
    test.in = (cvgroups.in == i)
    index = 1
    for (m in 1:nbaseModels) {
      for (l in 1:m) {
        rf.fit = randomForest(formula = allModels[[m]], data = fulldata.in[train.in,], mtry = l)
        rf.allpredictedCV.in[test.in,index] = predict(rf.fit, newdata = fulldata.in[test.in,], type = "prob")[,2]
        index = index +1
      }
    }
  }
    #create a vector to store classification error rates
  rf.allmisclass.in = rep(-1,36) 
    #calculate mis-class error rates for each of the 36 models.
  for(m in 1:36){
    cmat.in = table(ifelse(rf.allpredictedCV.in[,m]>0.50,"1","0"),fulldata.in$low)
    if(dim(cmat.in)[1] == 2){
      rf.allmisclass.in[m] = 1-((cmat.in[1,1] + cmat.in[2,2]) / sum(cmat.in))
    }else{
      rf.allmisclass.in[m] = 1-((cmat.in[1,1] + 0) / sum(cmat.in))
    }
  }
  #############################################
  ##   end INNER CV for random forest/       ##
  ## /start INNER CV for k-nearest neighbors ##
  #############################################
    #create storage for 170 predictions for 100 k-neighbor models.  
  knn.allpredictedCV.in = matrix(rep(NA,n.in*k.neighbors),ncol=k.neighbors)
  
    #capture predictions performing CV10 for 100 models and store in matrix.
  for (i in 1:k.in)  {
    test.in = (cvgroups.in == i)
    train.x.in = fulldata.in[!test.in,]
    test.x.in = fulldata.in[test.in,]

      #scale numeric test.in predictors.
    std.age.train.in = scale(train.x.in$age); std.ptl.train.in = scale(train.x.in$ptl)
    std.ftv.train.in = scale(train.x.in$ftv)
      #scale numeric train.in predictors
    std.age.test.in = scale(test.x.in$age, 
                           center = attr(std.age.train.in,"scaled:center"), 
                           scale = attr(std.age.train.in,"scaled:scale"))
    std.ptl.test.in = scale(test.x.in$ptl, 
                           center = attr(std.ptl.train.in,"scaled:center"), 
                           scale = attr(std.ptl.train.in,"scaled:scale"))
    std.ftv.test.in = scale(test.x.in$ptl, 
                           center = attr(std.ftv.train.in,"scaled:center"), 
                           scale = attr(std.ftv.train.in,"scaled:scale"))

      #rebuild training.in and testing.in dataframes for knn.
    std.train.x.in = data.frame(std.age.train.in,
                             train.x.in$race,
                             train.x.in$smoke,
                             std.ptl.train.in,
                             train.x.in$ht,
                             train.x.in$ui,
                             std.ftv.train.in)
    std.test.x.in = data.frame(std.age.test.in,
                             test.x.in$race,
                             test.x.in$smoke,
                             std.ptl.test.in,
                             test.x.in$ht,
                             test.x.in$ui,
                             std.ftv.test.in)
    
    for (m in 1:k.neighbors) {
      knn.allpredictedCV.in[test.in,m] = knn(std.train.x.in,std.test.x.in,fulldata.in$low[!test.in], k = m)
    }
  }
    #create storage for 100 miss-class error rates. 
  knn.allmisclass.in = rep(-1,k.neighbors)
  
    #calculate miss-class error rates for each of the 100 models.
  for(m in 1:k.neighbors){
    cmat.in = table(knn.allpredictedCV.in[,m],fulldata.in$low)
    if(dim(cmat.in)[1] == 2){
      knn.allmisclass.in[m] = 1-((cmat.in[1,1] + cmat.in[2,2]) / sum(cmat.in))
    }else{
      knn.allmisclass.in[m] = 1-((cmat.in[1,1] + 0) / sum(cmat.in))
    }
  }
  ############################################
  ## end INNER CV for k-nearest neighbors   ##
  ############################################
  #############################################
  ##  end INNER CV calc for both models/     ##
  ## /Start model fitting OUTER CV loop      ##
  ############################################# 

  #############################################
  ##        identify best model              ##
  #############################################  
    #combining CV10 error rates, and selecting index of lowest rate - that is best model.
  bestmodel.in = order(c(rf.allmisclass.in,knn.allmisclass.in))[1]
  
  ############################################
  ##        fit best model                  ##
  ############################################
  #now that we have identified best model, fit it to the outer CV10's training data.
    #if the model was a random forest, then we need to fit that with the right number of predictors and mtry.
    #Note: if the best model was a KNN, then we go right into prediction step.
  if (bestmodel.in <= 36) { # then best is one of random forest models
    if(bestmodel.in == 1){        #RF 1 predictor
      bestfit = randomForest(formula = allModels[[bestmodel.in]], data = traindata.out, mtry = bestmodel.in)
    } else if(bestmodel.in <=3){ #RF 2 predictor
      bestfit = randomForest(formula = allModels[[ceiling(bestmodel.in/nbaseModels)]], data = traindata.out, 
                             mtry = bestmodel.in-1)
    } else if(bestmodel.in <=6){ #RF 3 predictor
      bestfit = randomForest(formula = allModels[[ceiling(bestmodel.in/nbaseModels)]], data = traindata.out, 
                             mtry = bestmodel.in-3)
    } else if(bestmodel.in <=10){ #RF 4 predictor
      bestfit = randomForest(formula = allModels[[ceiling(bestmodel.in/nbaseModels)]], data = traindata.out, 
                             mtry = bestmodel.in-6)
    } else if(bestmodel.in <=15){ #RF 5 predictor
      bestfit = randomForest(formula = allModels[[ceiling(bestmodel.in/nbaseModels)]], data = traindata.out, 
                             mtry = bestmodel.in-10)
    } else if(bestmodel.in <=21){ #RF 6 predictor
      bestfit = randomForest(formula = allModels[[ceiling(bestmodel.in/nbaseModels)]], data = traindata.out, 
                             mtry = bestmodel.in-15)
    } else if(bestmodel.in <=28){ #RF 7 predictor
      bestfit = randomForest(formula = allModels[[ceiling(bestmodel.in/nbaseModels)]], data = traindata.out, 
                             mtry = bestmodel.in-21)
    } else                      { #RF 8 predictor
      bestfit = randomForest(formula = allModels[[ceiling(bestmodel.in/nbaseModels)]], data = traindata.out, 
                             mtry = bestmodel.in-28)
    }
  }
  ######################################################################
  ##  Making predictions w/best model on the "outer" holdout sample   ##
  ######################################################################  
    #keep track of best 2x CV models for curiosity.
  allbestmodels[j] = bestmodel.in
  
    #preform prediction using the outer CV10's test/validation set. Store in vector.
      #if model was 36 or less, use the model fitted with random forest.
      #else predict response using knn.
  if (bestmodel.in <= 36) {  # then best is one of random forest
    allpredictedCV.out[groupj.out] = ifelse(predict(bestfit,newdata = testdata.out, type = "prob")[,2]>0.50,2,1)
  } else {  
    allpredictedCV.out[groupj.out] = knn(std.train.x.out,std.test.x.out,fulldata.out$low[!groupj.out], k = bestmodel.in - 36)
  }
  ####################################
  ##   End of CV outer loop         ##
  ## (will recycle for k.out times) ##
  ####################################  
}
  #match the output of knn with the actual response classes.
allpredictedCV.out = ifelse(allpredictedCV.out=="1",0,1)
  #create confusion matrix and calculate error rate. 
cmat.best = table(allpredictedCV.out,birth$low); cmat.best
1-((cmat.best[1,1] + cmat.best[2,2]) / sum(cmat.best))
```