---
title: "Homework 2 R markdown"
author: "Josh Jarvey"
date: '`r Sys.Date()`'
output:
  word_document:
    fig_height: 4
    fig_width: 4.5
  pdf_document:
    fig_height: 4
    fig_width: 4.5
  html_document:
    fig_height: 4
    fig_width: 4.5
---


```{r, setup, include=FALSE}
require(mosaic)   # Load additional packages here 

# Some customization.  You can alter or delete as desired (if you know what you are doing).
#trellis.par.set(theme=theme.mosaic()) # change default color scheme for lattice
knitr::opts_chunk$set(
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
```

#### <span style="color:Blue">**Intellectual Property:**</span>  
These problems are the intellectual property of the instructors and may not be reproduced outside of this course.

#### <span style="color:Crimson">**Due Date:**</span>  
Tuesday, Sep 19, 2017 at 11:59 PM 

***  

##########################################################################
## <span style="color:DarkViolet">Problem 1:  Model Assessment  </span>
##########################################################################

<span style="color:DarkViolet">This problem practices application of proper model assessment techniques, with a multiple linear regression model.</span>

<span style="color:DarkViolet">Download the data set *Trees.csv* [from Lesson 2 on D2L] and read it into R.  Reference with description of the *original* measurements may be found at: </span> https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/trees.html

```{r echo=FALSE}
library(readr)
trees = read_csv("C:/Users/joshj/Documents/DS740-Data Mining and Machine Learning/Homework/Week 2/trees.csv")
```

<span style="color:DarkViolet">The general goal for this dataset is to predict Volume based on Girth and Height.  We will be fitting a predictive model using multiple linear regression.  The model is given below:
$Volume = \beta_0+\beta_1\cdot Girth +\beta_2\cdot Height+\beta_3\cdot Girth\cdot Height+\beta_4 \cdot Girth^2+\beta_5\cdot Girth^2\cdot Height$  
Note that there are five predictors, some of which are transformations of the original two variables Girth and Height, for predicting the value of the response variable Volume.</span>

### <span style="color:DarkViolet">Question 1</span> **<span style="color:Crimson">(2 points)</span>**
<span style="color:DarkViolet">Why is *Volume* the most reasonable response variable?  *Include real-world reasons (eg. physical practicalities) in your discussion.*</span>

Volume is the most reasonable response variable because as the girth of a tree expands, and the height of a tree grows, it is taking up more area in space. In a 3 dimensional world, this means more volume and therefore a natural relationship should exist between girth/height and volume.

<span style="color:green">**Text Answer**: </span>

***


### <span style="color:DarkViolet">Questions 2</span> **<span style="color:Crimson">(1 point)</span>**
<span style="color:DarkViolet">Use multiple linear regression fit the model to the full data set.  Identify the coefficient estimates ($\hat{\beta}_1$, $\hat{\beta}_2$, $\hat{\beta}_3$, $\hat{\beta}_4$, $\hat{\beta}_5$) for the five predictor terms.
Recall:  The t-statistic tests for the marginal significance of each term.  How many of the terms are marginally significant?  
</span>



```{r}
  #create a regression model using the predictors. None of these predictors are significant
model = lm(Volume ~ Girth + Height + GirthHeight + Girth2 + Girth2Height, data = trees)
summary(model)

```

<span style="color:green">**Multiple Choice Answer** </span>
**<span style="color:red">(AUTOGRADED on D2L)</span>**: 
0,    
1,  
2,  
3,  
4, or   
5


ZERO TERMS - although R2 is high. This might indicate a collinearity issue with the variables, and a VIF() score should be reviewed. 
*** 


<span style="color:DarkViolet">We now apply k-fold cross-validation to produce honest predictions, using the process outlined in the next several questions.</span>



### <span style="color:DarkViolet">Question 3</span> **<span style="color:Crimson">(1 point)</span>**
<span style="color:DarkViolet">Starting with:</span>

$\texttt{groups = c(rep(1:5,6),1)}$

<span style="color:DarkViolet"> Set R’s seed to 2:

```{r, echo=TRUE}
set.seed(2, sample.kind = "Rounding")
```
and then define define cvgroups (random groups for the cross-validation) using the sample() function.  
Enter your R code below.</span>

<span style="color:green">**Code Answer**: </span>
```{r, echo=TRUE}
# Question 3

#using the code provided, create a vector with the sequence 1:5, 6x times. There is 1 remaining observation above 30, so combine "1" at the end. This appears to be setting up 5 folds for CV (because each of the 31 observation's will get labeled 1, 2, 3, 4, or 5)
groups = c(rep(1:5,6),1)
  #count the sample size.
n = nrow(trees)
 #now, using sample, take a random sample, n times (31 times). Pull that sample from the "groups" vector, which has all the numbers 1:5 replicated many times. We can use this newly "randomly shuffled" vector to build our train/test groups.
cvgroups = sample(groups,n)


# Questions 4-5

  #creating an empty vector to store predicted values. This is just a vector of 31 zereos.
allpredictedCV = rep(0,n)

  #there are "5 folds", therefore we loop 5 times.
for (i in 1:5){
    #create a vector of True/False values, where TRUE values will be the hold out set for testing the model via prediction.
    #we'll go through the entire randomly created vector called "cvgroups", and where the element matches the current loop's 
    #iteration, this will be considered a "test" observation.
  test = (cvgroups == i)
    #next we will fit the model onto the tree's dataset, but we want to keep out the "test" dataset which was determined in the 
    #previous step. We use "subset =", and can cleverly use the inverse of the true/false vector via !test.
  fit = lm(formula = Volume~Girth+Height+GirthHeight+Girth2+Girth2Height,data=trees,subset=!test)
    #now that we've fit the model using the "training" data, we can use the model to predict the "test" data.
    #there are about 6 observations (for the most part) that are being predicted here in each iteration. 
    #For the returned predicted values, we want to store them the "allpredictedCV" vector, and do it in place of that iteration
  allpredictedCV[test] = predict.lm(fit,trees[test,])
}



# Question 6

```


### <span style="color:DarkViolet">Question 4</span> **<span style="color:Crimson">(1 point)</span>**
<span style="color:DarkViolet">Use the 5-fold CV method to assess the model fit. Provide the predicted y-value for the **first** observation: </span>

9.25 (actually, 9.33)

<span style="color:green">**Numeric Answer** </span>
**<span style="color:red">(AUTOGRADED on D2L)</span>**:


### <span style="color:DarkViolet">Question 5</span> **<span style="color:Crimson">(1 point)</span>**
<span style="color:DarkViolet">Use the 5-fold CV method to assess the model fit. Provide the predicted y-value for the **second** observation: </span>

10.06 (actually, 10.25)

<span style="color:green">**Numeric Answer** </span>
**<span style="color:red">(AUTOGRADED on D2L)</span>**:

***


### <span style="color:DarkViolet">Question 6</span> **<span style="color:Crimson">(2 points)</span>**

<span style="color:DarkViolet">Calculate and report the $CV_{(5)}$ based on the 5-fold cross-validation: </span>


```{r}
# Question 6
sum((allpredictedCV-trees$Volume)^2)/n
```

<span style="color:green">**Numeric Answer** </span>
**<span style="color:red">(AUTOGRADED on D2L)</span>**:

8.1536 (actually 19.74168)

### <span style="color:DarkViolet">Question 7</span> **<span style="color:Crimson">(3 points)</span>**
<span style="color:DarkViolet">Enter your R code for computing the $CV_{(5)}$ measure below.</span>

<span style="color:green">**Code Answer**: </span>
```{r echo=TRUE}

  #creating an empty vector to store predicted values. This is just a vector of 31 zereos.
allpredictedCV = rep(0,n)

  #there are "5 folds", therefore we loop 5 times.
for (i in 1:5){
    #create a vector of True/False values, where TRUE values will be the hold out set for testing the model via prediction.
    #we'll go through the entire randomly created vector called "cvgroups", and where the element matches the current loop's 
    #iteration, this will be considered a "test" observation.
  test = (cvgroups == i)
    #next we will fit the model onto the tree's dataset, but we want to keep out the "test" dataset which was determined in the 
    #previous step. We use "subset =", and can cleverly use the inverse of the true/false vector via !test.
  fit = lm(formula = Volume~Girth+Height+GirthHeight+Girth2+Girth2Height,data=trees,subset=!test)
    #now that we've fit the model using the "training" data, we can use the model to predict the "test" data.
    #there are about 6 observations (for the most part) that are being predicted here in each iteration. 
    #For the returned predicted values, we want to store them the "allpredictedCV" vector, and do it in place of that iteration
  allpredictedCV[test] = predict.lm(fit,trees[test,])
}
```


***  



**Bootstrapping**

<span style="color:DarkViolet"> We will now use the bootstrap to estimate variability of the coefficients.</span>

### <span style="color:DarkViolet">Question 8</span> **<span style="color:Crimson">(3 points)</span>**:

<span style="color:DarkViolet"> Program a function, making use of lm() to fit the linear regression model, that outputs the six coefficient estimates.   Set R’s seed to 2:

```{r, echo=TRUE}
set.seed(2, sample.kind = "Rounding")
```
and then use $\texttt{boot()}$ to produce R = 1000 bootstrap estimates for each of $\beta_0$, $\beta_1$, $\beta_2$, $\beta_3$, $\beta_4$, and $\beta_5$.  
Enter your R code below.</span>

<span style="color:green">**Code Answer**: </span>
```{r echo=TRUE}
# Question 8 

  #creating the custom function that takes in the dataset, and the indices (which will be determined by bootstrap re-sampling). 
  #This function returns the coefficients of the regression model fit, given that resampled dataset. 
lr.function <- function(inputdata, index){
  return(coef(lm(Volume ~ Girth + Height + GirthHeight + Girth2 + Girth2Height, data = inputdata, subset = index)))
}

  #load the bootstrap library
library(boot)
  #perform 1000 bootstrap iterations using the trees dataset and the fitting of the regression model.
boot(trees,lr.function,R=1000)


# Questions 9-14

```


### <span style="color:DarkViolet">Questions 9-14</span> **<span style="color:Crimson">(6 points, 1 each)</span>**:

<span style="color:DarkViolet">Use your bootstrap estimates to estimate the standard error, $SE(\beta_i)$, for each of i = 0, 1, 2, 3, 4, 5.</span>

<span style="color:green">**Numeric Answer**  </span>  
**<span style="color:red">(AUTOGRADED on D2L)</span>**:  
$SE(\hat{\beta_0}) =$   
$SE(\hat{\beta_1}) =$   
$SE(\hat{\beta_2}) =$   
$SE(\hat{\beta_3}) =$   
$SE(\hat{\beta_4}) =$   
$SE(\hat{\beta_5}) =$   


### <span style="color:DarkViolet">Question 15</span> **<span style="color:Crimson">(2 points)</span>**:

<span style="color:DarkViolet">The standard errors estimated from usual linear regression methods are shown in the R output below:</span>

$\texttt{Coefficients:				}$

$\texttt{Variable       Estimate  Std. Error  t value	 PR(>|t|)}$

$\texttt{(Intercept)	 48.914179	90.852925	 0.538	   0.595}$

$\texttt{Girth	       -8.228180	13.803580	-0.596	   0.556}$

$\texttt{Height		     -0.616152	 1.250446	-0.493	   0.626}$

$\texttt{GirthHeight	  0.103075	 0.180291	 0.572	   0.573}$

$\texttt{Girth2	        0.311160	 0.536379	 0.580	   0.567}$

$\texttt{Girth2Height	 -0.001764	 0.006621	-0.266	   0.792}$

<span style="color:DarkViolet">How do these values compare to the standard errors computed in the previous set of questions? </span>

<span style="color:green">**Multiple Choice Answer** </span>
**<span style="color:red">(AUTOGRADED on D2L)</span>**:  one of 

A) 	The estimates from usual linear regression methods are **greater**.

	
B)  The estimates from usual linear regression methods are **less**.

LESS THAN	
	
C) 	The two sets of estimates are about the **same**.


***

## Problem 2 - Model Selection

<span style="color:DarkViolet">This problem practices application of proper model selection techniques, with a multiple linear regression model.
We will continue working with the predictive model using multiple linear regression.  However, we will now consider selection between 6 possible models:</span>

<span style="color:DarkViolet">Model 1: 
$Volume = \beta_0+\beta_1\cdot Girth +\beta_2\cdot Height+\beta_3\cdot Girth\cdot Height+\beta_4 \cdot Girth^2+\beta_5\cdot Girth^2\cdot Height$  
</span>

<span style="color:DarkViolet">Model 2: 
$Volume = \beta_0+\beta_1\cdot Girth +\beta_2\cdot Height$  
</span>

<span style="color:DarkViolet">Model 3: 
$Volume = \beta_0+\beta_1\cdot Girth +\beta_2\cdot Height+\beta_3\cdot Girth\cdot Height$  
</span>

<span style="color:DarkViolet">Model 4: 
$Volume = \beta_0+\beta_1\cdot Girth +\beta_2\cdot Height+\beta_4 \cdot Girth^2+\beta_5\cdot Girth^2\cdot Height$  </span>

<span style="color:DarkViolet">Model 5: 
$Volume = \beta_0+\beta_4 \cdot Girth^2+\beta_5\cdot Girth^2\cdot Height$  
</span>

<span style="color:DarkViolet">Model 6: 
$Volume = \beta_0+\beta_5\cdot Girth^2\cdot Height$  
</span>

### <span style="color:DarkViolet">Questions 16-17</span> **<span style="color:Crimson">(2 points, 1 each)</span>**:

<span style="color:DarkViolet">Use LOOCV (note n = 31) method to calculate $CV_{(31)}$ for each of Models 1-6.  Report the $CV_{(31)}$ for Models 1 and 2.</span>

<span style="color:green">**Numeric Answer** </span>  
**<span style="color:red">(AUTOGRADED on D2L)</span>**:  
For Model 1, $CV_{(31)}$ =  8.96
For Model 2, $CV_{(31)}$ =  18.16
(use code space in next question)  

### <span style="color:DarkViolet">Question 18</span> **<span style="color:Crimson">(4 points)</span>**:

<span style="color:DarkViolet"> Enter your R code for computing the $CV_{(31)}$ measure for Model 6 below. </span>

<span style="color:green">**Possible Answer**: </span>
```{r echo=TRUE}
#Q16

  #creating models 1-6 based on the instruction above.
Model1 = (Volume ~ Girth + Height + GirthHeight + Girth2 + Girth2Height)
Model2 = (Volume ~ Girth + Height)
Model3 = (Volume ~ Girth + Height + GirthHeight)
Model4 = (Volume ~ Girth + Height + Girth2 + Girth2Height)
Model5 = (Volume ~ Girth2 + Girth2Height)
Model6 = (Volume ~ Girth2Height)

  #storing all 6 models in a list so they can be accessed within the for loop for model selection.
allModels = list(Model1,Model2,Model3,Model4,Model5,Model6)	

  #create an empty vector to store CV31 results of each model
allmodelCV = rep(NA,6)

	#Loop through the calculation of CV31 calculation 6x times, one for each model.
  #m will denote the current model from the allModels list object.
for (m in 1:6) {

    #for each iteration (i.e. model) create an empty vector to store the results of the 1 predicted value
  predictedCV = rep(0,n)
  
  for (i in 1:31){
      #next we will fit the model onto the tree's dataset, but we want to keep out the "test" dataset which is just the current 
      #i'th iteration since this is LOOV.
    fit = lm(formula = allModels[[m]],data=trees[-i,])
      #now that we've fit the model using the "training" data, we can use the model to predict the "test" data.
      #there are 1 observation that are being predicted here in each iteration. 
      #For the returned predicted values, we want to store them the "allpredictedCV" vector, and do it in place of that iteration
    predictedCV[i] = predict.lm(fit,trees[i,])
  }
    #once the CV31 process has taken place, calculate the CV31 score, and store it into the "allmodelCV" vector at the current m 
    #location. Each model will have its CV31 score calculated and stored, and reviewed in the further questions. 
  allmodelCV[m] = sum((predictedCV-trees$Volume)^2)/n
}

#Q17

#Q18



```


### <span style="color:DarkViolet">Question 19</span> **<span style="color:Crimson">(1 point)</span>**:

<span style="color:DarkViolet">Which model would you select based on the values of $CV_{(31)}$ for LOOCV? </span>

<span style="color:green">**Multiple Choice Answer** </span>
**<span style="color:red">(AUTOGRADED on D2L)</span>**:  one of  
Model 1,  
Model 2,  
Model 3,  
Model 4,  
Model 5, or  
Model 6  

***

MODEL 6 since it has the lowest CV31 value 


### <span style="color:DarkViolet">Question 20</span> **<span style="color:Crimson">(1 point)</span>**:

<span style="color:DarkViolet">Explain why you chose the model selected in the previous question. </span>

Model 6 has the lowest CV error value.

<span style="color:green">**Text Answer**: </span>

### <span style="color:DarkViolet">Questions 21-22</span> **<span style="color:Crimson">(2 points, 1 each)</span>**:

<span style="color:DarkViolet">Using the same split of the data into five sets as you performed in Problem 1, use 5-fold cross-validation method to calculate $CV_{(5)}$  for each of Models 1-6.  Report the $CV_{(5)}$  for Models 1 and 2.</span>

```{r}

#Q21


  #create an empty vector to store CV5 results of each model
allmodelCV = rep(NA,6)

	#Loop through the calculation of CV5 calculation 6x times, one for each model.
  #m will denote the current model from the allModels list object.
for (m in 1:6) {

  #creating an empty vector to store predicted values. This is just a vector of 31 zereos.
  allpredictedCV = rep(0,n)

    #there are "5 folds", therefore we loop 5 times.
  for (i in 1:5){
      #create a vector of True/False values, where TRUE values will be the hold out set for testing the model via prediction.
      #we'll go through the entire randomly created vector called "cvgroups", and where the element matches the current loop's 
      #iteration, this will be considered a "test" observation.
    test = (cvgroups == i)
      #next we will fit the model onto the tree's dataset, but we want to keep out the "test" dataset which was determined in the
      #previous step. We use "subset =", and can cleverly use the inverse of the true/false vector via !test.
    fit = lm(formula = allModels[[m]],data=trees,subset=!test)
      #now that we've fit the model using the "training" data, we can use the model to predict the "test" data.
      #there are about 6 observations (for the most part) that are being predicted here in each iteration. 
      #For the returned predicted values, we want to store them the "allpredictedCV" vector, and do it in place of that iteration
    allpredictedCV[test] = predict.lm(fit,trees[test,])
  }
      #once the CV5 process has taken place, calculate the CV5 score, and store it into the "allmodelCV" vector at the current 
      #m location. Each model will have its CV5 score calculated and stored, and reviewed in the further questions. 
  allmodelCV[m] = sum((allpredictedCV-trees$Volume)^2)/n
}

#Q22

```

<span style="color:green">**Numeric Answer** </span>  
**<span style="color:red">(AUTOGRADED on D2L)</span>**:  
For Model 1, $CV_{(5)}$ =  19.741684
For Model 2, $CV_{(5)}$ =  19.973142
(use code space above)  



### <span style="color:DarkViolet">Question 23</span> **<span style="color:Crimson">(1 point)</span>**:

<span style="color:DarkViolet">Which model would you select based on the values of $CV_{(5)}$ for 5-fold CV? </span>

<span style="color:green">**Multiple Choice Answer** </span>
**<span style="color:red">(AUTOGRADED on D2L)</span>**:  one of  
Model 1,  
Model 2,  
Model 3,  
Model 4,  
Model 5, or  
Model 6  

MODEL 6 is the choice.

### <span style="color:DarkViolet">Question 24</span> **<span style="color:Crimson">(2 points)</span>**:

<span style="color:DarkViolet">Considering the form of the model that was selected by cross-validation, why does this model make sense from a practical standpoint? </span>


A tree's height goes in the y dimension (i.e. it moves vertically, only one measurement), whereas the girth is in the x AND z axis in 3-dimensional space (its basically the circumference). These two are explicitly linked because the way a tree grows.

<span style="color:green">**Text Answer**: </span>

*** 


## Problem 3 - Model Assessment & Selection with KNN

<span style="color:DarkViolet"> This problem practices application of proper model assessment and selection techniques, with the kNN model. </span> 

<span style="color:DarkViolet"> **Important**:  Use the FNN library for fitting K-nearest neighbors, to obtain consistent answers.</span>

<span style="color:DarkViolet"> In this problem, you will once again use the K-nearest neighbors approach to analyze the gas mileage of cars.  You will use the **Auto** data set from the ISLR package, along with the two new variables, **weight.std** and **year.std** (standardized values of the weight and year), that you created in Homework 1: K-Nearest Neighbors.</span>




### <span style="color:DarkViolet">Question 25</span> **<span style="color:Crimson">(3 points)</span>**:

<span style="color:DarkViolet"> **Model assessment**   </span>
<span style="color:DarkViolet"> Starting with: </span>

$\texttt{groups = c(rep(1:10,39),1,2)}$

<span style="color:DarkViolet">  Set R’s seed to 2:

```{r, echo=TRUE}
set.seed(2, sample.kind = "Rounding")
library(FNN)
```
and use sample() to divide the data into ten sets.  Then use 10-fold cross-validation method to calculate $CV_{(10)}$  for 1-nearest neighbor regression. Remember to re-standardize each training set inside the cross-validation. Report the value.   </span>

<span style="color:green">**Numeric Answer** </span>  
**<span style="color:red">(AUTOGRADED on D2L)</span>**:  
<span style="color:DarkViolet"> $CV_{(10)}$ = </span>  
(use code space in next question)  


### <span style="color:DarkViolet">Question 26</span> **<span style="color:Crimson">(4 points)</span>**:

<span style="color:DarkViolet">Enter your R code for computing the $CV_{(10)}$ measure below. </span>

<span style="color:green">**Code Answer**: </span>
```{r echo=TRUE}
library(ISLR)
data("Auto")

groups = c(rep(1:10,39),1,2)

  #count the sample size.
n = nrow(Auto)

 #now, using sample, take a random sample, n times (392 times). Pull that sample from the "groups" vector, which has all the numbers 1:10 replicated many times. We can use this newly "randomly shuffled" vector to build our train/test groups.
cvgroups = sample(groups,n)


  #creating an empty vector to store predicted values. This is just a vector of 392 zereos.
predictedCV = rep(0,n)

    #there are "10 folds", therefore we loop 10 times.
for (i in 1:10){
      #create a vector of True/False values, where TRUE values will be the hold out set for testing the model via prediction.
      #we'll go through the entire randomly created vector called "cvgroups", and where the element matches the current loop's 
      #iteration, this will be considered a "test" observation.
  test = (cvgroups == i)
    
    
        #create 2 new dataframes pulling training data using the "non-test" values.
  train.x = Auto[!test,]
    #and pulling the validation data using JUST the test values. 
  valid.x = Auto[test,]
  
    #scale both the weight and year from the training dataset. Stored into their own variables.
  weight.train.std = scale(train.x$weight)
  year.train.std = scale(train.x$year)
  
  #scale the weight variable in the VALID dataset this time, but also make sure to use the same parameters for center/scale from the train set.
  weight.valid.std = scale(valid.x$weight, 
                           center = attr(weight.train.std,"scaled:center"), 
                           scale = attr(weight.train.std,"scaled:scale")
                           )
   
  #scale the year variable in the VALID dataset this time, but also make sure to use the same parameters for center/scale from the train set.
  year.valid.std = scale(valid.x$year, 
                           center = attr(year.train.std,"scaled:center"), 
                           scale = attr(year.train.std,"scaled:scale")
                           )
  
    #now that we have appropriately scaled variables, create a dataframe to put the x's together for both train and test sets.
  train.x.std = data.frame(weight.train.std,year.train.std)
  valid.x.std = data.frame(weight.valid.std,year.valid.std)
  
    #completing the knn regression analysis on MPG using the standardized year and weight variables. k = 1
    #store the predictions in the empty vector of zeroes, at the current iteration's spot.
    #repeat this process for each iteration of the 10 folds. 
  predictedCV[test] = knn.reg(train.x.std,valid.x.std,Auto$mpg[!test], k=1)$pred
}
      #finally, calculating the CV10 MSE between actual values of MPG in the predicted set vs. actual mpg values. 
sum((predictedCV-Auto$mpg)^2)/n  

```



### <span style="color:DarkViolet">Question 27</span> **<span style="color:Crimson">(1 point)</span>**:

<span style="color:DarkViolet">In general, how should the $CV_{(10)}$ value compare to the value of MSE (computed by reusing the same data used to fit the model)?</span>

<span style="color:green">**Multiple Choice Answer** </span>
**<span style="color:red">(AUTOGRADED on D2L)</span>**:  one of 
$CV_{(10)} > MSE$,  
$CV_{(10)} < MSE$, or  
$CV_{(10)} \approx MSE$

***

CV10 will larger because raw MSE should perform perfectly if you use the same data you trained with, to test with. It already knows what those data points are.


### <span style="color:DarkViolet">Question 28</span> **<span style="color:Crimson">(3 points)</span>**:

<span style="color:DarkViolet">Consider models 1-30 as the k-nearest neighbors regression for values of k from 1 to 30. Using the same split of the data into ten sets as you performed in the Model assessment section, use 10-fold cross-validation method to calculate CV(10) for each of Models 1-30; remember to re-standardize each training set inside the cross-validation. Make a plot of the CV(10) as a function of k.
Upload your plot to the Quiz question.  </span>

<span style="color:green">**Plot upload: </span>
```{r echo=FALSE}

  #setting up my k values
k = seq(1:30)
  #setting up an empty vector which will store all the CV values for each of the 30 models. 
allCV = numeric(length(k))

  #for each value of k, 1-30, iterate and perform CV10 using the knn.reg() model. 
for (j in 1:length(k)){

    #creating an empty vector to store predicted values. This is just a vector of 392 zereos.
  predictedCV = rep(0,n)
  
      #there are "10 folds", therefore we loop 10 times.
  for (i in 1:10){
      #create a vector of True/False values, where TRUE values will be the hold out set for testing the model via prediction.
      #we'll go through the entire randomly created vector called "cvgroups", and where the element matches the current loop's 
      #iteration, this will be considered a "test" observation.
    test = (cvgroups == i)
      
      
      #create 2 new dataframes pulling training data using the "non-test" values.
    train.x = Auto[!test,]
      #and pulling the validation data using JUST the test values. 
    valid.x = Auto[test,]
    
      #scale both the weight and year from the training dataset. Stored into their own variables.
    weight.train.std = scale(train.x$weight)
    year.train.std = scale(train.x$year)
    
      #scale the weight variable in the VALID dataset this time, but also make sure to use the same parameters for center/scale 
      #from the train set.
    weight.valid.std = scale(valid.x$weight, 
                             center = attr(weight.train.std,"scaled:center"), 
                             scale = attr(weight.train.std,"scaled:scale")
                             )
     
      #scale the year variable in the VALID dataset this time, but also make sure to use the same parameters for center/scale 
      #from the train set.
    year.valid.std = scale(valid.x$year, 
                             center = attr(year.train.std,"scaled:center"), 
                             scale = attr(year.train.std,"scaled:scale")
                             )
    
      #now that we have appropriately scaled variables, create a dataframe to put the x's together for both train and test sets.
    train.x.std = data.frame(weight.train.std,year.train.std)
    valid.x.std = data.frame(weight.valid.std,year.valid.std)
    
      #completing the knn regression analysis on MPG using the standardized year and weight variables. k = 1
      #store the predictions in the empty vector of zeroes, at the current iteration's spot.
      #repeat this process for each iteration of the 10 folds. 
    predictedCV[test] = knn.reg(train.x.std,valid.x.std,Auto$mpg[!test], k=k[j])$pred
  }
    #finally, calculating the CV10 MSE between actual values of MPG in the predicted set vs. actual mpg values.
    #this CV10 value needs to be stored in the current iteration's "K" value, so it can be plotted once complete with 30 models.
  allCV[j] = sum((predictedCV-Auto$mpg)^2)/n  
}

  #finally, both K and the allCV values are plotted, and we observe k=20 as the low point for MSE. 
plot(k,allCV,type="l")

```



### <span style="color:DarkViolet">Question 29</span> **<span style="color:Crimson">(2 points)</span>**:

<span style="color:DarkViolet">Which k (number of nearest neighbors) would you select based on the values of $CV_{(10)}$ for 10-fold CV?

 </span>

<span style="color:green">**Numeric (Integer) Answer** </span>  
**<span style="color:red">(AUTOGRADED on D2L)</span>**:  


k=20

### <span style="color:DarkViolet">Question 30 </span> **<span style="color:Crimson">(2 points)</span>**:

<span style="color:DarkViolet">Explain why you chose the k value specified in the previous question. *Comment on both model predictive ability and model complexity.*</span>

<span style="color:green">**Text Answer**: </span>


I chose k=20 because this appears to have produced the lowest CV10 error value. CV10 values for k leading into 20 are on the decline, whereas values after 20 start to show an increase in CV10 error.One could argue that k=14 may be optimal, as it produces another low spot in the CV10 values. The difference between k=14 and k=20 isnt by much, but k=20 is the lowest CV10 value. 
