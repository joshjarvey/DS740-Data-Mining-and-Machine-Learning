require(mosaic)   # Load additional packages here
# Some customization.  You can alter or delete as desired (if you know what you are doing).
#trellis.par.set(theme=theme.mosaic()) # change default color scheme for lattice
knitr::opts_chunk$set(
tidy=FALSE,     # display code as typed
size="small")   # slightly smaller font for code
library(readr)
income = read_csv("C:/Users/joshj/Documents/DS740-Data Mining and Machine Learning/Homework/Week 3/Wisconsin_income.csv")
library(readr)
income = read_csv("C:/Users/joshj/Documents/DS740-Data Mining and Machine Learning/Homework/Week 3/Wisconsin_income.csv")
View(income)
#Citizenship, Class of worker, Language spoken at home, Marital status, Sex, Disability, Race, Hispanic
income$CIT2 = as.factor(income$CIT2)
income$COW = as.factor(income$COW)
income$LANX = as.factor(income$LANX)
income$MAR = as.factor(heart$ECG)
#Citizenship, Class of worker, Language spoken at home, Marital status, Sex, Disability, Race, Hispanic
income$CIT2 = as.factor(income$CIT2)
income$COW = as.factor(income$COW)
income$LANX = as.factor(income$LANX)
income$MAR = as.factor(income$MAR)
income$SEX = as.factor(income$SEX)
income$DIS = as.factor(income$DIS)
income$RAC = as.factor(income$RAC)
income$Hispanic = as.factor(income$Hispanic)
View(income)
hist(income$PERNP)
hist(income$WKHP)
hist(income$JWMNP)
hist(income$PERNP, main = "Distribution of Incomes", xlab = "Income (in thousands $)")
hist(income$WKHP)
hist(income$JWMNP)
hist(income$PERNP, main = "Distribution of Incomes", xlab = "Income (in thousands $)")
hist(income$WKHP, main = "Distribution Hours worked per Week", xlab = "Hours")
hist(income$JWMNP, main = "Distribution Travel Time to Work", xlab = "Travel time (in minutes)")
View(income)
View(income)
#creating histograms to check distribution of variables
hist(income$PERNP, main = "Distribution of Incomes", xlab = "Income (in thousands $)")
hist(income$WKHP, main = "Distribution Hours worked per Week", xlab = "Hours")
hist(income$JWMNP, main = "Distribution Travel Time to Work", xlab = "Travel time (in minutes)")
#both incomes and travel times look to be right-skewed, so we will log transform them.
income$PERNP = log(income$PERNP)
income$JWMNP = log(income$JWMNP)
library(readr)
income = read_csv("C:/Users/joshj/Documents/DS740-Data Mining and Machine Learning/Homework/Week 3/Wisconsin_income.csv")
#Citizenship, Class of worker, Language spoken at home, Marital status, Sex, Disability, Race, Hispanic to factor variables.
income$CIT2 = as.factor(income$CIT2)
income$COW = as.factor(income$COW)
income$LANX = as.factor(income$LANX)
income$MAR = as.factor(income$MAR)
income$SEX = as.factor(income$SEX)
income$DIS = as.factor(income$DIS)
income$RAC = as.factor(income$RAC)
income$Hispanic = as.factor(income$Hispanic)
#creating histograms to check distribution of variables
hist(income$PERNP, main = "Distribution of Incomes", xlab = "Income (in thousands $)")
hist(income$WKHP, main = "Distribution Hours worked per Week", xlab = "Hours")
hist(income$JWMNP, main = "Distribution Travel Time to Work", xlab = "Travel time (in minutes)")
#both incomes and travel times look to be right-skewed, so we will log transform them.
income$logPERNP = log(income$PERNP)
income$logJWMNP = log(income$JWMNP)
install.packages("LEAP")
install.packages("leaps")
library(leaps)
#regfit.full = regsubsets()
library(leaps)
regfit.full = regsubsets(logPERNP~.-PERP-JWMNP,data = income)
library(leaps)
regfit.full = regsubsets(logPERNP~.-PERNP-JWMNP,data = income)
library(leaps)
regfit.full = regsubsets(logPERNP~.-PERNP-JWMNP,data = income, nvmax = 41)
library(leaps)
regfit.full = regsubsets(logPERNP~.-PERNP-JWMNP,data = income, nvmax = 41)
plot(regfit.full)
# Question 4
plot(regfit.full, scale = "adjr2")
set.seed(3, sample.kind = "Rounding")
# Define a predict() function for regsubsets objects
predict.regsubsets <- function(object, newdata, id, ...){
form = as.formula(object$call[[2]])
mat = model.matrix(form, newdata)
coefi = coef(object, id=id)
xvars = names(coefi)
mat[ , xvars] %*% coefi
} # end function predict.regsubsets
# Question 8
n = nrow(income)
k = 10 #using 10-fold cross-validation
groups = c(rep(1:k,floor(n/k)),1:(n-floor(n/k)*k))  #produces list of group labels
cvgroups = sample(groups,n)
# Question 4
#plotting the all subsets using the adj. r2 measure
plot(regfit.full, scale = "adjr2")
regfit.summary = summary(regfit.full)
View(regfit.summary)
# Question 4
#plotting the all subsets using the adj. r2 measure
plot(regfit.full, scale = "adjr2")
regfit.summary = summary(regfit.full)
which.min(regfit.summary$adjr2)
regfit.summary$adjr2
which.max(regfit.summary$adjr2)
regfit.summary$adjr2
# Question 4
#plotting the all subsets using the adj. r2 measure
plot(regfit.full, scale = "adjr2")
regfit.summary = summary(regfit.full)
which.max(regfit.summary$adjr2)
which.min(regfit.summary$bic)
coef(regfit.full,35)
which.min(regfit.summary$bic)
coef(regfit.full,18)
# Define a predict() function for regsubsets objects
predict.regsubsets <- function(object, newdata, id, ...){
form = as.formula(object$call[[2]])
mat = model.matrix(form, newdata)
coefi = coef(object, id=id)
xvars = names(coefi)
mat[ , xvars] %*% coefi
} # end function predict.regsubsets
#setting n to be the number of observations in the dataset
n = nrow(income)
#using 10-fold cross-validation
k = 10
#produces list of group labels from 1-10
groups = c(rep(1:k,floor(n/k)),1:(n-floor(n/k)*k))
#setting up our cross-validation groups be randomizing each observation's CV group's label
cvgroups = sample(groups,n)
#row = number of variables per each model, column = which fold. This matrix will store each model's (from 1 to 41) CV error
group.error = matrix(,nr=41, nc=k)
#setting up a for loop to perform cross-validation
for(i in 1:k){
#using the current iteration of the CV fold, set up a "test" hold out sample.
test = (cvgroups == i)
#perform all subsets regression, using all the data EXCEPT the test hold out sample, allow 41 variables.
cv.fit = regsubsets(logPERNP~.-PERNP-JWMNP, data=income[!test,], nvmax=41)
for(j in 1:41){
y.pred = predict(cv.fit, newdata = income[test,], id=j)
group.error[j, i] = mean((income$logPERNP[test]-y.pred)^2)
} # end iter over model size
} # end iter over folds
# Define a predict() function for regsubsets objects
predict.regsubsets <- function(object, newdata, id, ...){
form = as.formula(object$call[[2]])
mat = model.matrix(form, newdata)
coefi = coef(object, id=id)
xvars = names(coefi)
mat[ , xvars] %*% coefi
} # end function predict.regsubsets
#setting n to be the number of observations in the dataset
n = nrow(income)
#using 10-fold cross-validation
k = 10
#produces list of group labels from 1-10
groups = c(rep(1:k,floor(n/k)),1:(n-floor(n/k)*k))
#setting up our cross-validation groups be randomizing each observation's CV group's label
cvgroups = sample(groups,n)
#row = number of variables per each model, column = which fold. This matrix will store each model's (from 1 to 41) CV error
group.error = matrix(,nr=41, nc=k)
#setting up a for loop to perform cross-validation
for(i in 1:k){
#using the current iteration of the CV fold, set up a "test" hold out sample.
test = (cvgroups == i)
#perform all subsets regression, using all the data EXCEPT the test hold out sample, allow 41 variables.
cv.fit = regsubsets(logPERNP~.-PERNP-JWMNP, data=income[!test,], nvmax=41)
for(j in 1:41){
y.pred = predict.regsubsets(cv.fit, newdata = income[test,], id=j)
group.error[j, i] = mean((income$logPERNP[test]-y.pred)^2)
} # end iter over model size
} # end iter over folds
View(group.error)
# Define a predict() function for regsubsets objects
predict.regsubsets <- function(object, newdata, id, ...){
form = as.formula(object$call[[2]])
mat = model.matrix(form, newdata)
coefi = coef(object, id=id)
xvars = names(coefi)
mat[ , xvars] %*% coefi
} # end function predict.regsubsets
#setting n to be the number of observations in the dataset
n = nrow(income)
#using 10-fold cross-validation
k = 10
#produces list of group labels from 1-10
groups = c(rep(1:k,floor(n/k)),1:(n-floor(n/k)*k))
#setting up our cross-validation groups be randomizing each observation's CV group's label
cvgroups = sample(groups,n)
#row = number of variables per each model, column = which fold. This matrix will store each model's (from 1 to 41) CV error
group.error = matrix(,nr=41, nc=k)
#setting up a for loop to perform cross-validation
for(i in 1:k){
#using the current iteration of the CV fold, set up a "test" hold out sample.
test = (cvgroups == i)
#perform all subsets regression, using all the data EXCEPT the test hold out sample, allow 41 variables.
cv.fit = regsubsets(logPERNP~.-PERNP-JWMNP, data=income[!test,], nvmax=41)
#now for each one of these 41 models, use the custom predict function on the test hold-out sample.
#calculate the CV error and store it into the group.error matrix for the current model (id=j).
#repeat this process for all folds of the cross validation.
for(j in 1:41){
#use custom predict function - pass reg subsets object, the hold out dataset, and the current model (j).
y.pred = predict.regsubsets(cv.fit, newdata = income[test,], id=j)
#calculate the CV error and store it in the appropate spot of the matrix
group.error[j, i] = mean((income$logPERNP[test]-y.pred)^2)
}
}
View(regfit.summary)
View(cv.fit)
View(group.error)
plot(regfit.full)
View(group.error)
y.pred = predict.regsubsets(cv.fit, newdata = income[test,], id=40)
coef(cv.fit, id=40)
coef(regfit.full,35)
# Question 12
library(ISLR)
# Question 12
library(ISLR)
data(Auto)
force(Auto)
# Question 12
library(ISLR)
data(Auto)
Auto$highMPG = ifelse(MPG > median(MPG),1,0)
force(Auto)
# Question 12
library(ISLR)
data(Auto)
Auto$highMPG = ifelse(mpg > median(mpg),1,0)
force(Auto)
# Question 12
library(ISLR)
data(Auto)
Auto$highMPG = ifelse(mpg > median(Auto$mpg),1,0)
# Question 12
library(ISLR)
data(Auto)
Auto$highMPG = ifelse(Auto$mpg > median(Auto$mpg),"1","0")
# Question 12
library(ISLR)
data(Auto)
Auto$highMPG = as.factor(ifelse(Auto$mpg > median(Auto$mpg),"1","0"))
# Question 12
#load the dataset
library(ISLR)
data(Auto)
#create a new variable called "highMPG". Set to 1 if the current observation's mpg is greater than the median of all mpgs. Treat as factor.
Auto$highMPG = as.factor(ifelse(Auto$mpg > median(Auto$mpg),"1","0"))
plot(Auto)
plot(Auto)
# Question 14
fit = glm(highMPG~.-mpg-name,data = Auto, family = "binomial")
# Question 15
# Question 14
#fitting a logistic regression model with the response of my new "highMPG" variable, using all the data except mpg and name.
fit = glm(highMPG~.-mpg-name,data = Auto, family = "binomial")
summary(fit)
# Question 15
# Question 14
#fitting a logistic regression model with the response of my new "highMPG" variable, using all the data except mpg and name.
fit = glm(highMPG~.-mpg-name,data = Auto, family = "binomial")
# Question 15
library(car)
# Question 14
#fitting a logistic regression model with the response of my new "highMPG" variable, using all the data except mpg and name.
fit = glm(highMPG~.-mpg-name,data = Auto, family = "binomial")
# Question 15
library(car)
vif(fit)
set.seed(3, sample.kind = "Rounding")
# Question 16
#setting n to be the number of observations in the dataset
n = nrow(Auto)
#using 10-fold cross-validation
k = 10
#produces list of group labels from 1-10
groups = c(rep(1:k,floor(n/k)),1:(n-floor(n/k)*k))
#setting up our cross-validation groups be randomizing each observation's CV group's label
cvgroups = sample(groups,n)
#initializing an empty vector to store the CV error
predictvals = rep(-1,n)
#setting up a for loop to perform cross-validation
for(i in 1:k){
#using the current iteration of the CV fold, set up a "test" hold out sample.
test = (cvgroups == i)
#perform logistic regression, using all the data EXCEPT the test hold out sample.
fit = glm(highMPG~.-mpg-name,data = Auto[!test,], family = "binomial")
predictvals[test] = predict(fit, Auto[test,], type = "response")
}
Auto2 = Auto[,-name]
Auto2 = Auto[,-"name"]
Auto2 = Auto[-"name"]
Auto2 = Auto["name"]
Auto2 = -Auto["name"]
Auto2 = Auto[-c("name")]
Auto2 = Auto[,-c("name")]
Auto2 = Auto[,c("name")]
Auto2 = Auto[c("name"),]
Auto2 = Auto[,c(-"name")]
Auto2 = Auto[,-name]
Auto2 = Auto[,-Auto$name]
Auto2 = Auto[,1:8]
Auto2 = Auto[,1:8,9]
Auto2 = Auto[,c(1:8,9)]
Auto2 = Auto[,c(1:8,10)]
# Question 16
Auto2 = Auto[,c(1:8,10)]
#setting n to be the number of observations in the dataset
n = nrow(Auto2)
#using 10-fold cross-validation
k = 10
#produces list of group labels from 1-10
groups = c(rep(1:k,floor(n/k)),1:(n-floor(n/k)*k))
#setting up our cross-validation groups be randomizing each observation's CV group's label
cvgroups = sample(groups,n)
#initializing an empty vector to store the CV error
predictvals = rep(-1,n)
#setting up a for loop to perform cross-validation
for(i in 1:k){
#using the current iteration of the CV fold, set up a "test" hold out sample.
test = (cvgroups == i)
#perform logistic regression, using all the data EXCEPT the test hold out sample.
fit = glm(highMPG~.-mpg-name,data = Auto2[!test,], family = "binomial")
predictvals[test] = predict(fit, Auto2[test,], type = "response")
}
# Question 16
Auto2 = Auto[,c(1:8,10)]
#setting n to be the number of observations in the dataset
n = nrow(Auto2)
#using 10-fold cross-validation
k = 10
#produces list of group labels from 1-10
groups = c(rep(1:k,floor(n/k)),1:(n-floor(n/k)*k))
#setting up our cross-validation groups be randomizing each observation's CV group's label
cvgroups = sample(groups,n)
#initializing an empty vector to store the CV error
predictvals = rep(-1,n)
#setting up a for loop to perform cross-validation
for(i in 1:k){
#using the current iteration of the CV fold, set up a "test" hold out sample.
test = (cvgroups == i)
#perform logistic regression, using all the data EXCEPT the test hold out sample.
fit = glm(highMPG~.-mpg,data = Auto2[!test,], family = "binomial")
predictvals[test] = predict(fit, Auto2[test,], type = "response")
}
predictvals
#Question 17
library(pROC)
#Question 17
library(pROC)
myroc = roc(response = Auto2$highMPG, predictor = predictvals)
plot(myroc)
#Question 17
library(pROC)
myroc = roc(response = Auto2$highMPG, predictor = predictvals)
plot(myroc)
auc(myroc)
#Question 17
#load the pROC library
library(pROC)
#creating the roc object.
myroc = roc(response = Auto2$highMPG, predictor = predictvals)
#calculating the AUC. 0.9679 in this case.
auc(myroc)
# Question 18
#plotting the roc object to visualize the ROC curve
plot(myroc)
View(myroc)
myroc$thresholds
myroc$specificities
myroc$sensitivities
# Question 18
#plotting the roc object to visualize the ROC curve
plot(myroc)
#The model has a good amount of area under the curve, thus performing much better than "random guessing". The curve jumps up sharply right away in sensitivity
myroc$sensitivities
plot(myroc$sensitivities)
plot(myroc$specificities)
plot(myroc$specificities)
plot(myroc$sensitivities)
plot(myroc$specificities,myroc$sensitivities)
plot(myroc$thresholds,myroc$sensitivities,type="l",col="red")
lines(myroc$thresholds,myroc$specificities,col="green")
plot(myroc)
