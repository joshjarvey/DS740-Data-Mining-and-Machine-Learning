#read in the dataset
heart = read.csv("C:/Users/joshj/Documents/DS740-Data Mining and Machine Learning/Webwork/Week 5/Heart_disease_Cleveland.csv")
#removing 6 observations due to missing data
heart = heart[-c(88,167,193,267,288,303),]
#transforming STdpress with a log transformation. Add +1 to keep from infinity.
heart$log.STdepress = log(heart$STdepress+1)
#remove the non-transformed STdepressed from the dataset.
heart = heart[,-10]
#setting as factor variables.
heart$Sex = as.factor(heart$Sex)
heart$ChestPain = as.factor(heart$ChestPain)
heart$HighBloodSugar = as.factor(heart$HighBloodSugar)
heart$ECG = as.factor(heart$ECG)
heart$ExerAngina = as.factor(heart$ExerAngina)
heart$Slope = as.factor(heart$Slope)
heart$Thal = as.factor(heart$Thal)
heart$DiseaseStatus = as.factor(heart$DiseaseStatus)
#finding the correlation
cor.matrix = abs(cor(heart[,c(1,4,5,8,11,14)]))
which.max(cor.matrix<1)
#read in the dataset
heart = read.csv("C:/Users/joshj/Documents/DS740-Data Mining and Machine Learning/Webwork/Week 5/Heart_disease_Cleveland.csv")
#removing 6 observations due to missing data
heart = heart[-c(88,167,193,267,288,303),]
#transforming STdpress with a log transformation. Add +1 to keep from infinity.
heart$log.STdepress = log(heart$STdepress+1)
#remove the non-transformed STdepressed from the dataset.
heart = heart[,-10]
#setting as factor variables.
heart$Sex = as.factor(heart$Sex)
heart$ChestPain = as.factor(heart$ChestPain)
heart$HighBloodSugar = as.factor(heart$HighBloodSugar)
heart$ECG = as.factor(heart$ECG)
heart$ExerAngina = as.factor(heart$ExerAngina)
heart$Slope = as.factor(heart$Slope)
heart$Thal = as.factor(heart$Thal)
heart$DiseaseStatus = as.factor(heart$DiseaseStatus)
#finding the correlation
abs(cor(heart[,c(1,4,5,8,11,14)]))
#read in the dataset
heart = read.csv("C:/Users/joshj/Documents/DS740-Data Mining and Machine Learning/Webwork/Week 5/Heart_disease_Cleveland.csv")
#removing 6 observations due to missing data
heart = heart[-c(88,167,193,267,288,303),]
#transforming STdpress with a log transformation. Add +1 to keep from infinity.
heart$log.STdepress = log(heart$STdepress+1)
#remove the non-transformed STdepressed from the dataset.
heart = heart[,-10]
#setting as factor variables.
heart$Sex = as.factor(heart$Sex)
heart$ChestPain = as.factor(heart$ChestPain)
heart$HighBloodSugar = as.factor(heart$HighBloodSugar)
heart$ECG = as.factor(heart$ECG)
heart$ExerAngina = as.factor(heart$ExerAngina)
heart$Slope = as.factor(heart$Slope)
heart$Thal = as.factor(heart$Thal)
heart$DiseaseStatus = as.factor(heart$DiseaseStatus)
#finding the correlation of the numeric variables
#next use abs()
abs(cor(heart[,c(1,4,5,8,11)]))
#read in the dataset
heart = read.csv("C:/Users/joshj/Documents/DS740-Data Mining and Machine Learning/Webwork/Week 5/Heart_disease_Cleveland.csv")
#removing 6 observations due to missing data
heart = heart[-c(88,167,193,267,288,303),]
#transforming STdpress with a log transformation. Add +1 to keep from infinity.
heart$log.STdepress = log(heart$STdepress+1)
#remove the non-transformed STdepressed from the dataset.
heart = heart[,-10]
#setting as factor variables.
heart$Sex = as.factor(heart$Sex)
heart$ChestPain = as.factor(heart$ChestPain)
heart$HighBloodSugar = as.factor(heart$HighBloodSugar)
heart$ECG = as.factor(heart$ECG)
heart$ExerAngina = as.factor(heart$ExerAngina)
heart$Slope = as.factor(heart$Slope)
heart$Thal = as.factor(heart$Thal)
heart$DiseaseStatus = as.factor(heart$DiseaseStatus)
#finding the correlation of the numeric variables
#next use abs()
round(abs(cor(heart[,c(1,4,5,8,11)])),3)
library(MASS)
data("Boston")
force(Boston)
library(MASS)
data("Boston")
BostonNew = Boston
#load in the dataset and create a new dataframe for manipulation
library(MASS)
data("Boston"); BostonNew = Boston
#add log transformed versions of the crim and zn predictors.
BostonNew$log.crim = log(BostonNew$crim)
BostonNew$log.zn = log(BostonNew$zn+1)
#remove the non-transformed STdepressed from the dataset.
BostonNew = BostonNew[,-c(1,2)]
#load in the dataset and create a new dataframe for manipulation
library(MASS)
data("Boston"); BostonNew = Boston
#add log transformed versions of the crim and zn predictors.
BostonNew$log.crim = log(BostonNew$crim)
BostonNew$log.zn = log(BostonNew$zn+1)
#and remove the non-transformed crim & zn from the dataset.
BostonNew = BostonNew[,-c(1,2)]
#set chas as a factor.
BostonNew$chas = as.factor(BostonNew$chas)
x = model.matrix(BostonNew[,c(1:11,14)])
BostonNew[,c(1:11,14)]
y = BostonNew[,13]
bodyfat = read.csv("bodyfat.csv")
model.matrix(BodyFatSiri~.,data=bodyfat)[,-(1:4)]
x = model.matrix(log.crim~.,data = BostonNew)
View(x)
install.packages("glmnet")
library(glmnet)
lambdalist = exp((1200:-1200)/100)  # order large to small
lambdalist
lambdalist = .001:1
lambdalist = seq(.001,1, by=0.001)
lambdalist
lambdalist = seq(1,.001, by=-0.001)# order large to small
lambdalist
RRfit = glmnet(x,y,alpha = 0,lambda = lambdalist)
x = model.matrix(log.crim~.,data = BostonNew)[,-1]
View(x)
RRfit = glmnet(x,y,alpha = 0,lambda = lambdalist)
RRfit = glmnet(x,y,alpha = 0,lambda = 0.05)
coef(RRfit)
coef(RRfit,s=0.05)
#fit the Lasso regression model (alpha = 1), and use the 1000x lambda vector
LRfit = glmnet(x,y,alpha = 1,lambda = lambdalist)
#check coefficients where s=0.05
coef(LRfit,s=0.05)
#fit the Elastic Net regression model (alpha = 0.50), and use the 1000x lambda vector
EN50fit = glmnet(x,y,alpha = 0.50,lambda = lambdalist)
#check coefficients where s=0.05
coef(EN50fit,s=0.05)
coef(LRfit,s=0.05)
coef(EN50fit,s=0.05)
coef(RRfit,s=0.05)
n = nrow(BostonNew)
k = 10
set.seed(5, sample.kind = "Rounding")
k = 10
n = nrow(BostonNew)
groups=c(rep(1:k,floor(n/k)),(1:(n-k*floor(n/k))))
cvgroups = sample(groups,n)
groups
#RIDGE REGRESSION
#fit the ridge regression model (alpha = 0), and use the 1000x lambda vector
cvRRfit = cv.glmnet(x, y, alpha = 0, lambda = lambdalist, nfolds=k, foldid=cvgroups)
#check coefficients where s=0.05
coef(cvRRfit,s=0.05)
cvRRfit$cvm
lowestCVM = min(cvRRfit$cvm)
View(cvRRfit)
lowestCVM = order(cvRRfit$cvm)
lowestCVM = min(cvRRfit$cvm)
orderedbyCVMRank = order(cvRRfit$cvm)
whichlowestcvRR = order(cvRRfit$cvm)[1]; min(cvRRfit$cvm)
bestlambdaRR = lambdalist[whichlowestcvRR]; bestlambdaRR
#LASSO REGRESSION
#fit the LASSO regression model (alpha = 1), and use the 1000x lambda vector
cvLRfit = cv.glmnet(x, y, alpha = 1, lambda = lambdalist, nfolds=k, foldid=cvgroups)
#find the minimum CV error rate based on all the different lambda values.
min(cvLRfit$cvm)
#order the CV error rates by lowest to greatest, and find the indice where its the lowest
whichlowestcvLR = order(cvLRfit$cvm)[1]
#use that indicie to find the lambda value.
bestlambdaLR = lambdalist[whichlowestcvLR]
bestlambdaLR
#ELASTIC NET REGRESSION
#fit the Elastic Net regression model (alpha = 0), and use the 1000x lambda vector
cvENfit = cv.glmnet(x, y, alpha = 0.50, lambda = lambdalist, nfolds=k, foldid=cvgroups)
#find the minimum CV error rate based on all the different lambda values.
min(cvENfit$cvm)
#order the CV error rates by lowest to greatest, and find the indice where its the lowest
whichlowestcvEN = order(cvENfit$cvm)[1]
#use that indicie to find the lambda value.
bestlambdaEN = lambdalist[whichlowestcvEN]
bestlambdaEN
library(boot)
View(x)
library(boot)
# do the same for the best-fitting (Elastic Net) model
beta.fn.ENET = function(inputdata,index) {
yboot = inputdata[index,1]
xboot = inputdata[index,-1]
ENETfitboot = glmnet(xboot, yboot, alpha = 0.50,lambda=lambdalist)
return(coef(ENETfitboot,s=bestlambdaENET50)[,1])
}
ENETbootoutput = boot(cbind(y,x),beta.fn.ENET,R=1000)
ENETbootoutput = boot(cbind(y,x),beta.fn.ENET,R=1000)
# do the same for the best-fitting (Elastic Net) model
beta.fn.ENET = function(inputdata,index) {
yboot = inputdata[index,1]
xboot = inputdata[index,-1]
ENETfitboot = glmnet(xboot, yboot, alpha = 0.50,lambda=0.035)
return(coef(ENETfitboot,s=bestlambdaEN)[,1])
}
ENETbootoutput = boot(cbind(y,x),beta.fn.ENET,R=1000)
print(ENETbootoutput)
# do the same for the best-fitting (Ridge Regression) model
beta.fn.RR = function(inputdata,index) {
yboot = inputdata[index,1]
xboot = inputdata[index,-1]
RRfitboot = glmnet(xboot, yboot, alpha = 0,lambda=0.016)
return(coef(RRfitboot,s=bestlambdaRR)[,1])
}
# do the same for the best-fitting (LASSO) model
beta.fn.LR = function(inputdata,index) {
yboot = inputdata[index,1]
xboot = inputdata[index,-1]
LRfitboot = glmnet(xboot, yboot, alpha = 1,lambda=0.021)
return(coef(LRfitboot,s=bestlambdaLR)[,1])
}
RRbootoutput = boot(cbind(y,x),beta.fn.RR,R=1000)
LRbootoutput = boot(cbind(y,x),beta.fn.LR,R=1000)
ENETbootoutput = boot(cbind(y,x),beta.fn.ENET,R=1000)
View(LRbootoutput)
# compare variability of coefs
data.frame(cbind(round(apply(RRbootoutput$t,2,sd),4),
round(apply(LRbootoutput$t,2,sd),4),
round(apply(ENETbootoutput$t,2,sd),4)),
row.names=c("intercept",names(BostonNew)[1:12,14]))
ENETbootoutput$t
# compare variability of coefs
data.frame(cbind(round(apply(RRbootoutput$t,2,sd),4),
round(apply(LRbootoutput$t,2,sd),4),
round(apply(ENETbootoutput$t,2,sd),4)),
row.names=c("intercept",names(BostonNew)[1:12,14]))
# compare variability of coefs
data.frame(cbind(round(apply(RRbootoutput$t,2,sd),4),
round(apply(LRbootoutput$t,2,sd),4),
round(apply(ENETbootoutput$t,2,sd),4)),
row.names=c("intercept",names(BostonNew)[c(1:12,14)]))
RRbootoutput
round(apply(RRbootoutput$t,2,sd),4)
# compare variability of coefs
data.frame(cbind(round(apply(RRbootoutput$t,2,sd),4),
round(apply(LRbootoutput$t,2,sd),4),
round(apply(ENETbootoutput$t,2,sd),4)),
row.names=c("intercept",names(BostonNew)[c(1:12,14)]))
coef(RRfitboot,s=bestlambdaRR)[,1]
coef(RRfitboot,s=bestlambdaRR)
coef(EN50fit,s=0.05)
View(ENETbootoutput)
RRbootoutput$t
cbind(y,x)
# compare variability of coefs
data.frame(cbind(round(apply(RRbootoutput$t,2,sd),4),
round(apply(LRbootoutput$t,2,sd),4),
round(apply(ENETbootoutput$t,2,sd),4)),
row.names=c("intercept",names(BostonNew)[c(1:12,14)]))
data.frame(cbind(round(apply(RRbootoutput$t,2,sd),4),
round(apply(LRbootoutput$t,2,sd),4),
round(apply(ENETbootoutput$t,2,sd),4)),
row.names=c("intercept",names(BostonNew)[c(1:12,14)]))
# compare variability of coefs
data.frame(cbind(round(apply(RRbootoutput$t,2,sd),4),
round(apply(LRbootoutput$t,2,sd),4),
round(apply(ENETbootoutput$t,2,sd),4)),
row.names=c("intercept",names(BostonNew)[c(1)]))
data.frame(cbind(round(apply(RRbootoutput$t,2,sd),4),
round(apply(LRbootoutput$t,2,sd),4),
round(apply(ENETbootoutput$t,2,sd),4)),
row.names=c("intercept",names(BostonNew)[c(1:12,14)]))
require(mosaic)   # Load additional packages here
# Some customization.  You can alter or delete as desired (if you know what you are doing).
#trellis.par.set(theme=theme.mosaic()) # change default color scheme for lattice
knitr::opts_chunk$set(
tidy=FALSE,     # display code as typed
size="small")   # slightly smaller font for code
trees = read.csv("C:/Users/joshj/Documents/DS740-Data Mining and Machine Learning/Homework/Week 5/Trees.csv")
View(trees)
#read in the data
trees = read.csv("C:/Users/joshj/Documents/DS740-Data Mining and Machine Learning/Homework/Week 5/Trees.csv")[,-1]
fit = lm(Volume~.,data = trees)
#read in the data
trees = read.csv("C:/Users/joshj/Documents/DS740-Data Mining and Machine Learning/Homework/Week 5/Trees.csv")[,-1]
#fit the multiple linear regression model
fit = lm(Volume~.,data = trees)
#check summary
summary(fit)
lambdalist = c((1:100)/100)
library(glmnet)
#setting up the list of lambda values
lambdalist = c((1:100)/100)
y = trees[,1]
x = trees[,-1]
LRfit = glmnet(x,y,alpha = 1, lambda = 0.1)
View(x)
x = model.matrix(Volume~.,data = trees)[,-1]
View(x)
library(glmnet)
#setting up the list of lambda values
lambdalist = c((1:100)/100)
y = trees[,1]
x = model.matrix(Volume~.,data = trees)[,-1]
LRfit = glmnet(x, y, alpha = 1, lambda = 0.1)
library(glmnet)
#setting up the list of lambda values
lambdalist = c((1:100)/100)
y = trees[,1]
x = model.matrix(Volume~.,data = trees)[,-1]
LRfit = glmnet(x, y, alpha = 1, lambda = lambdalist)
coef(LRfit,s=0.1)
lambdalist = order(c((1:100)/100))
lambdalist = sort(c((1:100)/100))
lambdalist = sort(c((1:100)/100),desc)
library(glmnet)
#setting up the list of lambda values
lambdalist = sort(c((1:100)/100),decreasing = T)
#pull volume as y
y = trees[,1]
#pull the remaining variables into a matrix as the x's.
x = model.matrix(Volume~.,data = trees)[,-1]
#fit the LASSO regression
LRfit = glmnet(x, y, alpha = 1, lambda = lambdalist)
#pull the coefficients
coef(LRfit,s=0.1)
library(ISLR)
library(ISLR)
data("College")
force(College)
View(College)
help("College")
library(ISLR)
data("College")
help("College")
summary(College)
212/777
View(College)
College$log.Enroll = log(College$Enroll)
College$log.Apps = log(College$Apps)
College$log.Accept = log(College$Accept)
College$log.F.Undergrad = log(College$F.Undergrad)
College$log.P.Undergrad = log(College$P.Undergrad)
#adding log transformed versions of these skewed variables.
College$log.Enroll = log(College$Enroll)
College$log.Apps = log(College$Apps)
College$log.Accept = log(College$Accept)
College$log.F.Undergrad = log(College$F.Undergrad)
College$log.P.Undergrad = log(College$P.Undergrad)
hist(College$log.Enroll)
#adding log transformed versions of these skewed variables.
College$log.Enroll = log(College$Enroll)
College$log.Apps = log(College$Apps)
College$log.Accept = log(College$Accept)
College$log.F.Undergrad = log(College$F.Undergrad)
College$log.P.Undergrad = log(College$P.Undergrad)
hist(College$log.Enroll, main = "Distribution of Enrollments", xlab = "Enrollments (log scale)")
#adding log transformed versions of these skewed variables.
College$log.Enroll = log(College$Enroll)
College$log.Apps = log(College$Apps)
College$log.Accept = log(College$Accept)
College$log.F.Undergrad = log(College$F.Undergrad)
College$log.P.Undergrad = log(College$P.Undergrad)
hist(College$log.Enroll, main = "Distribution of College Enrollments", xlab = "Enrollments (log scale)")
#adding log transformed versions of these skewed variables.
College$log.Enroll = log(College$Enroll)
College$log.Apps = log(College$Apps)
College$log.Accept = log(College$Accept)
College$log.F.Undergrad = log(College$F.Undergrad)
College$log.P.Undergrad = log(College$P.Undergrad)
hist(College$log.Enroll, main = "Distribution of College Enrollments", xlab = "Number of Enrollments (log scale)")
cor(College$log.Enroll,College[,c("Expend")])
cor(College$log.Enroll,College[,c("Expend","log.Accept","log.P.Undergrad","perc.alumni")])
cor(College$log.Enroll,College[,c("Expend","log.Accept","log.P.Undergrad","perc.alumni","Personal")])
abs(cor(College$log.Enroll,College[,c("Expend","log.Accept","log.P.Undergrad","perc.alumni","Personal")]))
cor(College)
View(College)
cor(College[,-1])
y = College[,19]
y = College[,19]
x = model.matrix(log.Enroll~.,data = College)2
y = College[,19]
x = model.matrix(log.Enroll~.,data = College)
View(x)
y = College[,19]
x = model.matrix(log.Enroll~.,data = College)[,-1]
View(x)
library(glmnet)
#extract y as the log.enroll
y = College[,19]
#extract all the remaining variables as the x's.
x = model.matrix(log.Enroll~.,data = College)[,-1]
lambdalist = seq(1:0.001, by = 0.001)
lambdalist = seq(1:0.001, by = -0.001)
lambdalist = seq(1,0.001, by = -0.001)
library(glmnet)
#extract y as the log.enroll
y = College[,19]
#extract all the remaining variables as the x's.
x = model.matrix(log.Enroll~.,data = College)[,-1]
lambdalist = seq(1,0.001, by = -0.001)
LRfit = glmnet(x,y,alpha = 1,lambda = lambdalist)
coef(LRfit)
library(glmnet)
#extract y as the log.enroll
y = College[,19]
#extract all the remaining variables as the x's.
x = model.matrix(log.Enroll~.,data = College)[,-1]
#setting up a list of lambda values in desc order
lambdalist = seq(1,0.001, by = -0.001)
#fitting the LASSO model.
LRfit = glmnet(x,y,alpha = 1,lambda = lambdalist)
coef(LRfit,s=0.02)
coef(LRfit,s=0.03)
coef(LRfit,s=0.05)
coef(LRfit,s=0.08)
coef(LRfit,s=0.02)
View(x)
library(glmnet)
#extract y as the log.enroll
y = College[,19]
#extract all the remaining variables as the x's.
x = model.matrix(log.Enroll~.-Apps-Accept-F.Undergrad-P,Undergrad,data = College)[,-1]
library(glmnet)
#extract y as the log.enroll
y = College[,19]
#extract all the remaining variables as the x's.
x = model.matrix(log.Enroll~.-Apps-Accept-F.Undergrad-P.Undergrad,data = College)[,-1]
#setting up a list of lambda values in desc order
lambdalist = seq(1,0.001, by = -0.001)
#fitting the LASSO model.
LRfit = glmnet(x,y,alpha = 1,lambda = lambdalist)
#checking coefficients
coef(LRfit,s=0.02)
coef(LRfit,s=0.03)
coef(LRfit,s=0.05)
coef(LRfit,s=0.08)
View(x)
#checking coefficients
coef(LRfit,s=0.02)
coef(LRfit,s=0.03)
coef(LRfit,s=0.05)
coef(LRfit,s=0.08)
coef(LRfit,s=0.05)
coef(LRfit,s=0.08)
coef(LRfit,s=0.02)
coef(LRfit,s=0.08)
View(x)
library(glmnet)
#extract y as the log.enroll
y = College[,19]
#extract all the remaining variables as the x's.
x = model.matrix(log.Enroll~.-Enroll-Apps-Accept-F.Undergrad-P.Undergrad,data = College)[,-1]
#setting up a list of lambda values in desc order
lambdalist = seq(1,0.001, by = -0.001)
#fitting the LASSO model.
LRfit = glmnet(x,y,alpha = 1,lambda = lambdalist)
#checking coefficients
coef(LRfit,s=0.02)
coef(LRfit,s=0.03)
coef(LRfit,s=0.05)
coef(LRfit,s=0.08)
View(x)
coef(LRfit,s=0.02)
coef(LRfit,s=0.03)
coef(LRfit,s=0.05)
coef(LRfit,s=0.08)
coef(LRfit,s=0.05)
coef(LRfit,s=0.03)
coef(LRfit,s=0.02)
#load the glmnet library
library(glmnet)
#create a vector of lambda's between 0.001 to 1, by 0.001 increments. Ordered greatest to least.
lambdalist = seq(1,.001, by=-0.001)
#RIDGE REGRESSION
#fit the ridge regression model (alpha = 0), and use the 1000x lambda vector
cvENfit = cv.glmnet(x, y, alpha = 0.75, lambda = lambdalist, nfolds=k, foldid=cvgroups)
#set seed for reproducibility
set.seed(5, sample.kind = "Rounding")
#create groups of 10 folds, with remainder 1 through 7
groups = c(rep(1:10,77),(1:7))
#randomize the groups without replacement.
cvgroups = sample(groups,777)
#set number of cross validation folds
k = 10
#set sample size
n = nrow(College)
#load the glmnet library
library(glmnet)
#create a vector of lambda's between 0.001 to 1, by 0.001 increments. Ordered greatest to least.
lambdalist = seq(1,.001, by=-0.001)
#RIDGE REGRESSION
#fit the ridge regression model (alpha = 0), and use the 1000x lambda vector
cvENfit = cv.glmnet(x, y, alpha = 0.75, lambda = lambdalist, nfolds=k, foldid=cvgroups)
#plot(lambdalist,cvENfit$)
plot(lambdalist,cvENfit$cvm)
plot(lambdalist,cvENfit$cvm, main = "CV Error vs. Lambda", xlab = "Lambda")
#set seed for reproducibility
set.seed(5, sample.kind = "Rounding")
#create groups of 10 folds, with remainder 1 through 7
groups = c(rep(1:10,77),(1:7))
#randomize the groups without replacement.
cvgroups = sample(groups,777)
#set number of cross validation folds
k = 10
#set sample size
n = nrow(College)
#load the glmnet library
library(glmnet)
#create a vector of lambda's between 0.001 to 1, by 0.001 increments. Ordered greatest to least.
lambdalist = seq(1,.001, by=-0.001)
#RIDGE REGRESSION
#fit the ridge regression model (alpha = 0), and use the 1000x lambda vector
cvENfit = cv.glmnet(x, y, alpha = 0.75, lambda = lambdalist, nfolds=k, foldid=cvgroups)
plot(lambdalist,cvENfit$cvm, main = "10-fold Cross Validation Error vs. Lambda", xlab = "Lambda", ylab = "CV Error")
#find the minimum CV error rate based on all the different lambda values.
min(cvENfit$cvm)
whichlowestcvEN = order(cvENfit$cvm)[1]
bestlambdaEN = lambdalist[whichlowestcvEN]
bestlambdaEN
